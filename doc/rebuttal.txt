AC rebuttal
==========

We are happy to provide clarifications to the questions posed. We enclose them below.

"mostly reinventing SVI"
SVI computes the posterior distribution of hidden variables given data by maximizing the ELBO. Please note that we do not have hidden variables in our formulation; "x" are the variables of our objective f(x), not data. We are interested in maximizing the free energy F(x,\gamma) as defined in Eqn. 4. This is the *free energy of the energy landscape* (defined via a Gibbs distribution). This is thus unrelated to marginal likelihood or variational ELBO where one can impose entropic constraints on the posterior distribution of the *hidden variables* via a prior.

"I suspect it'd become clear that you are maximizing the marginal likelihood"
Please note that we are not maximizing the marginal likelihood, as discussed above.

"how is this better than SGLD", "bizarre that you compare against Adam instead of SGLD"
SGLD is an MCMC algorithm that draws samples from a given distribution. If the step-size goes to zero slowly enough, akin to all MCMC algorithms, it converges to the maximum of the likelihood in exponentially long time-scales (see algorithms like SGHMC [CFG14], Santa [CCG15] etc. that train Bayesian neural networks using MCMC algorithms). Note that SGLD does not optimize the marginal likelihood because there is no notion of hidden variables in vanilla SGLD. We provide a brief review of SGLD in Appendix A of our paper.

We do not know of any results in the literature that train large deep networks such as the one used for CIFAR to get competitive error rates using SGLD. We would like to emphasize that Entropy-SGD simply uses MCMC sampling to estimate Eqn. 7 but it is unrelated to SGLD otherwise.

On the other hand, Adam is an algorithm for computing the maximum of a the likelihood of data given parameters or (equivalently, minimize the loss function). Entropy-SGD is an algorithm designed for minimizing the loss function f(x), in particular, it is not an MCMC algorithm that draws samples from the likelihood. It however does not explicitly do so and instead maximizes the local entropy. We therefore compare Entropy-SGD with state-of-the-art algorithms for training deep networks like Adam and SGD.

"frustrating that you discuss free energy and entropy without precise definitions"
Local entropy (local free energy) is formally defined in Def. 1 (Eqn. 4) but it is already introduced on pg. 2 in the Introduction. The discussion towards the end of Sec. 3 (pg. 5, first line) defines the classical entropy of a Gibbs distribution. The beginning of Sec. 3 based on Fig. 2 is intended to explain things to the reader at an intuitive level before proceeding to the formal definitions which are nevertheless present.

Free energy and classical entropy are related by the informal description "free energy = internal energy - temperature x entropy". Formally, the relation is
    F(\beta) = U(\beta) - \frac{1}{\beta} S(\beta)
where the log-partition function (free energy) is defined as F(\beta) = -\beta^{-1} \log Z(\beta), the internal energy is U(\beta) = \partial_\beta (\beta F(\beta)) and S(\beta) = \beta^2 \partial_\beta (F(\beta)) is the classical entropy.

Note that the above equation is about the entire Gibbs distribution, in our work we define a "local free energy" F(x, \beta) via the modified Gibbs distribution in Eqn. 3. We can add this discussion to the paper.

[CFG14] Chen, Tianqi, Emily B. Fox, and Carlos Guestrin. "Stochastic Gradient Hamiltonian Monte Carlo." ICML. 2014.
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient mcmc and stochastic optimization." arXiv:1512.07962 (2015).


----------------------------------------------------------------------

Title: SVI does not resemble Entropy-SGD without a "moving prior", a feature of our scheme

To address the objections 1) and 3) let \Xi denote the dataset, z denote the weights and x be the parameters of a variational distribution q_x(z). The ELBO can then be written as
(i) \log p(\Xi)
        \geq E_{z \sim q_x(z)} [\log p(\Xi | z)] - KL(q_x(z) || p(z))
and maximized with respect to x. The distribution p(z) is a fixed (parameter-free) prior, which one has to postulate.

On the other hand, Eqn. 4 in the paper can be used to write the log of local entropy as:
(ii) \log F(x,\gamma)
        = \log \int_{z \in Z} e^{-f(z; \Xi) - \gamma/2 |x-z|^2} dz
        \geq \int_{z \in Z} [-f(z; \Xi) - \gamma/2 |x-z|^2] dz;
where f(z) = -\log p(\Xi | z). We are unaware of a general way to choose a prior p(z) and a variational family q_x(z) that that makes (i) resemble (ii), and therefore interpret our method as “integrating out […] the posterior over neural network parameters.” The only way we can do so is to pick a specific “prior" that depends on the parameter x (hence, not really a prior). For instance, one could choose a uniform variational family (say, q_x(z) \propto constant for |x-z| \leq C and zero otherwise) and a Gaussian “prior" with mean x (\log p(z) = -\gamma/2 |x-z|^2) to make (ii) resemble ELBO. In this case p(z) would not be fixed, but it would “move” along with the current iterate x.

This "moving prior" is a crucial feature of our proposed algorithm. The gradient of local entropy (Eqn. 7 in the paper) clarifies this further:
    dF  = -\gamma (x - E_{z \sim r(z; x)} [z]);
where the distribution r(z; x) is
    r(z; x) \propto p(\Xi | z) \exp(-\gamma/2 |x-z|^2);
i.e. it contains a data likelihood term with a prior that "moves" along with the current iterate x.

Concerning the relation to SGLD, consider Belief Propagation (BP). Our proposed algorithm relates to the "focusing-Belief Propagation" variant (fBP), rather than the standard one [BBC16]. The difference between BP and fBP is analogous to that between SGLD and Entropy-SGD: the latter operates on a transformation of the energy landscape of the former, exploiting local entropic effects. This difference is crucial and indeed related to the "moving prior" of the previous discussion; plain SGLD (or BP) can only trade energy for entropy via the temperature parameter which does not allow for direct use of any geometric information of the landscape and does not help with narrow minima.

In view of your comment 2), we also implemented SGLD for LeNet on MNIST and All-CNN-BN on CIFAR and will add the following results to our paper: After a hyper-parameter search, the best we obtained were a test error of LeNet on 0.63 \pm 0.1% on MNIST after 300 epochs and 9.89 \pm 0.11% on All-CNN-BN after 500 epochs. Disregarding the slow convergence of SGLD, its generalization error is slightly worse than the results in our paper, viz. 0.48% with Entropy-SGD on LeNet (0.51% with SGD) and 8.65% with Entropy-SGD on All-CNN-BN (8.3% with SGD). For comparison, the authors in [CCF15] report an error of 0.71% with SGLD on MNIST with a slightly larger network (0.47% with Santa), there are no results in the literature where MCMC methods perform comparably to SGD on larger networks.

[BBC16] Baldassi, Carlo, et al. "Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes." PNAS (2016).
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient MCMC and stochastic optimization." arXiv:1512.07962 (2015).

-------------------------------------------------------------------------

Title: Re: Is the Hessian-vector product actually expensive?

Comment 3) above is for HVP of the loss function of a general deep network, not local entropy. Indeed, HVPs can be computed using forward differencing with two back-props. Such an approximation is susceptible to numerical errors --- especially in high dimensions [P94, M10]. One typically averages the HVP over many samples in the dataset which is expensive, for instance, the authors in [LSP93] average over a few hundred samples, which roughly translates to 5-10x the time required for one iteration of vanilla SGD. More accurate algorithms like that of [P94] also require this averaging over samples.

One could argue that accuracy in the approximation of HVP does not matter in practice for purposes of training; what matters is only the local curvature at a scale commensurate with the typical weight updates. The perturbation vector for computing HVPs using back-props thus needs to be chosen carefully. If so, indeed, approximate computation of HVP can be considered cheap.

[LSP93] LeCun, Yann, Patrice Y. Simard, and Barak Pearlmutter. "Automatic learning rate maximization by on-line estimation of the Hessian’s eigenvectors." NIPS (1993).
[P94] Pearlmutter, Barak A. "Fast exact multiplication by the Hessian." Neural computation 6.1 (1994): 147-160.
[M10] Martens, James. "Deep learning via Hessian-free optimization." ICML (2010).


-----------------------------------------------------------------------


---------------------------
MAIN REBUTTAL
 
We thank the reviewers and the area chair for their insightful feedback. We have incorporated all comments into our current draft. We first discuss the updates to the paper and address common questions raised by the reviewers. We have also posted individual comments to the reviewers to address specific questions.

Updates
=====

a) We have updated the experimental section of the paper with new experiments on MNIST, CIFAR-10 and two datasets on RNNs (PTB and char-LSTM).

b) We have modified the algorithm to introduce a technique called "scoping". This increases the scope parameter \gamma as training progresses instead of fixing it and has the effect of exploring the parameter space in the beginning of training (Sec. 4.3). As a result of this, we can now train all our networks with only the local entropy loss instead of treating it as a regularizer (Eqn. 6).

c) For a fair comparison of the training time, we now plot the error curves in Figs. 4, 5 and 6 against the "effective number of epochs", i.e., the number of epochs of Entropy-SGD is multiplied by the number of Langevin iterations L (we set L=1 for SGD/Adam). Thus the x-axis is a direct measure of the wall-clock time agnostic to the underlying implementation and is proportional to the number of back-props as suggested.

We obtain significant speed-ups with respect to our earlier results due to scoping and the wall-clock training time for all our networks with Entropy-SGD is now comparable to SGD/Adam. In fact, Entropy-SGD is almost twice as fast as SGD on our experiments on RNNs and also obtains a better generalization error (cf. Fig. 6). The acceleration for CNNs on MNIST and CIFAR-10 is about 20%.

Table 1 (page 11) summarizes the experimental section of the paper.

d) Improved exposition of the algorithm in Sec. 4.2 that includes intuition for hyper-parameter tuning. We have expanded the discussion of experiments in Sec. 5.3, 5.4 to provide more details and insights that relate to the energy landscape of deep networks.

e) Appendix C discusses the possible connections to variational inference (this is the same material as the discussion with the AC below). Sec. C.1 presents an experimental comparison of local entropy vs. SGLD. We note here that our results using Entropy-SGD for both CNNs and RNNs are much better than vanilla SGLD in significantly smaller (~3-5x) wall-clock times.

Response to the reviewers:
================

>> smoothing of the original loss vs. local entropy
We discuss this in detail in the related work in Sec. 2 and Appendix C. While smoothing the original loss function using convolutions or averaging the gradient over perturbations of weights reduces the ruggedness of the energy landscape, it does not help with sharp, narrow valleys. Local entropy introduces a measure that focuses on wide local minima in the energy landscape (cf. Fig. 2 which has a "global" minimum at a wide valley); this is the primary reason for its efficacy. Smoothing the loss function can also, for instance, generate an artificial local minimum between two close by sharp valleys, which is detrimental to generalization.

>> unrealistic eigenvalue assumption in Sec. 4.3
We have clarified this point in Remark 4. Our analysis employs an assumption that the Hessian \nabla^2 f(x) does not have eigenvalues in the set [-2\gamma-c, c] for some c > 0. This is admittedly unrealistic, for instance, the eigenspectrum of the Hessian in Fig. 1 has a large fraction of its eigenvalues almost zero. Let us note though that Fig. 1 is plotted at a local minimum, from our experiments, the eigenspectrum is less sparse in the beginning of training.

We would like to remark that the bound on uniform stability in Thm. 3 by Hardt et al. assumes global conditions on the smoothness of the loss function; one imagines that Eqn. 9 remains qualitatively the same (in particular, with respect to the number of training iterations) even if this assumption is violated to an extent before convergence happens. Obtaining a rigorous generalization bound without this assumption requires a dynamical analysis of SGD and seems out of reach currently.

--------------------
Area chair:

>> You should make the connections clearer and more explicit
We have updated the paper to include the above discussion about SVI and added experiments comparing with SGLD in Appendix C (also see the discussion in the last paragraph of Sec. 2). As already remarked in our previous comment, characterizing local entropy as "SVI with a moving prior" would be inaccurate as their relation is just conceptual and not rigorous.

>> change x-axis for error curves
We have updated the experiments in the paper and now plot all error curves against the "effective number of epochs", i.e., the number of epochs for Entropy-SGD is multiplied by L (we set L=1 for SGD/Adam). Note that this is proportional to the number of back-props. Please see the "updates to the paper" comment above for more details.

>> Exact HVP is easy to compute
The comment regarding [P94] above discusses this. Computing the exact HVP requires averaging over the entire dataset, i.e., ~4x the complexity of one epoch.

------------------
AnonReviewer4:

Please see the "updates to the paper" comment above for the response to the questions regarding (i) the eigenvalue assumption and, (ii) thorough experimental results.

>> how to reconcile "Entropy-SGD obtains better generalization but results in lower cross-entropy loss"
The confusion is probably caused by our omission, it should read "... lower cross-entropy loss on the training set" (fixed now). This experimental observation suggests that there exist wide valleys that are deeper in the energy landscape that also generalize well; Entropy-SGD manages to find them while SGD gets stuck at a higher loss.

>> \rho = 0.01 on CIFAR-10 but \rho = 0 on MNIST
We have modified the algorithm to only train with the local entropy objective, i.e., \rho = 0 always (cf. Eqn. 6). Using scoping we can obtain better results on all our networks both in terms of generalization error and wall-clock time.

>> discuss similarities to Hochreiter and Schmidhuber '97 [HS97]
We have expanded our discussion of [HS97] in Sec. 2 to include this discussion: While the motivations are exactly the same, similarities with their exact formulation are only conceptual in nature, e.g., in a flat minimum, the local entropy is a direct measure of the width of the valley which is similar to their usage of Hessian. The Gibbs variant to averaging in weight space in their analysis (Eqn. 33, pg. 22 of [HS97]) is similar to the averaging in Eqn. 7 of our paper. 

We agree with the reviewer that the elaborate analysis of generalization of [HS97] using the Gibbs formalism is a promising direction. In our case, we benefit from similar elaborate and technical results introduced for uniform stability [BE02].

>> experiments are on a toy example
Please note that our results on CIFAR-10 are without any data augmentation, the best result for this is 6.55% error using ELU units by [CUH15]. To our knowledge, our baseline on CIFAR-10 (7.71 \pm 0.19% error with SGD), is the best reported result for the popular All-CNN-C network [S14], which is a medium-sized model with about 1.6 million weights. The largest model we have experimented with is an LSTM on the PTB dataset with 66 million weights and we achieve better test perplexity than the original authors [ZSV14] in half as much wall-clock time.

[HS97] Hochreiter, S. and Schmidhuber, J. (1997), Flat Minima.
[BE02] Olivier Bousquet and Andre Elisseeff. Stability and generalization. JMLR, 2002.
[CUH15] Clevert, Djork-Arné, Thomas Unterthiner, and Sepp Hochreiter. "Fast and accurate deep network learning by exponential linear units (elus)." arXiv:1511.07289 (2015).
[S14] Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." arXiv:1412.6806 (2014).
[ZSV14] Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. "Recurrent neural network regularization." arXiv:1409.2329 (2014).

------------------
Csaba Szepesvari:

Please see the "updates to the paper" comment above for the response to the questions regarding (i) the eigenvalue assumption, (ii) thorough experimental results and, (iii) smoothing the loss vs. local entropy.

>> discussion of Baldassi et. al. '16 in related work
We have expanded the discussion of local entropy previously introduced by Baldassi et. al. in Sec. 2 and included due citations in Sec. 1 and 3 as well. This paper builds upon their work and generalizes it to models with continuous variables, deep networks in particular. After posting the first version of this paper, we have been collaborating with Baldassi et. al. for experiments on scoping techniques and they have co-authored this version of the paper.

------------------
AnonReviewer1:

>> Eqn. 8 should have f(x')
Thanks, it is fixed now.

>> experiments on RNNs
Thanks for suggesting this. We have included results for word-level and character-level text prediction on PTB and char-LSTM with War and Peace. We not only obtain a better test perplexity than a competitive baseline with SGD, but also train in about half as much wall-clock time.

Please see the "updates to the paper" comment above for more details.

------------------
AnonReviewer2:

>> Thm. 3 does not have the constant c in Hardt et. al. '15
We set the constant c of Hardt et. al., to 1 in our statement to avoid confusion with our constant, with is also called c (our statement has the learning rate \eta_t \leq 1/t instead of their original \eta_t \leq c/t). This does not change the result qualitatively.

Please see the "updates to the paper" comment above for the response to the questions regarding (i) the eigenvalue assumption, (ii) thorough experimental results and, (iii) smoothing the original loss vs. local entropy.