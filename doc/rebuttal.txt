AC rebuttal
==========

We are happy to provide clarifications to the questions posed. We enclose them below.

"mostly reinventing SVI"
SVI computes the posterior distribution of hidden variables given data by maximizing the ELBO. Please note that we do not have hidden variables in our formulation; "x" are the variables of our objective f(x), not data. We are interested in maximizing the free energy F(x,\gamma) as defined in Eqn. 4. This is the *free energy of the energy landscape* (defined via a Gibbs distribution). This is thus unrelated to marginal likelihood or variational ELBO where one can impose entropic constraints on the posterior distribution of the *hidden variables* via a prior.

"I suspect it'd become clear that you are maximizing the marginal likelihood"
Please note that we are not maximizing the marginal likelihood, as discussed above.

"how is this better than SGLD", "bizarre that you compare against Adam instead of SGLD"
SGLD is an MCMC algorithm that draws samples from a given distribution. If the step-size goes to zero slowly enough, akin to all MCMC algorithms, it converges to the maximum of the likelihood in exponentially long time-scales (see algorithms like SGHMC [CFG14], Santa [CCG15] etc. that train Bayesian neural networks using MCMC algorithms). Note that SGLD does not optimize the marginal likelihood because there is no notion of hidden variables in vanilla SGLD. We provide a brief review of SGLD in Appendix A of our paper.

We do not know of any results in the literature that train large deep networks such as the one used for CIFAR to get competitive error rates using SGLD. We would like to emphasize that Entropy-SGD simply uses MCMC sampling to estimate Eqn. 7 but it is unrelated to SGLD otherwise.

On the other hand, Adam is an algorithm for computing the maximum of a the likelihood of data given parameters or (equivalently, minimize the loss function). Entropy-SGD is an algorithm designed for minimizing the loss function f(x), in particular, it is not an MCMC algorithm that draws samples from the likelihood. It however does not explicitly do so and instead maximizes the local entropy. We therefore compare Entropy-SGD with state-of-the-art algorithms for training deep networks like Adam and SGD.

"frustrating that you discuss free energy and entropy without precise definitions"
Local entropy (local free energy) is formally defined in Def. 1 (Eqn. 4) but it is already introduced on pg. 2 in the Introduction. The discussion towards the end of Sec. 3 (pg. 5, first line) defines the classical entropy of a Gibbs distribution. The beginning of Sec. 3 based on Fig. 2 is intended to explain things to the reader at an intuitive level before proceeding to the formal definitions which are nevertheless present.

Free energy and classical entropy are related by the informal description "free energy = internal energy - temperature x entropy". Formally, the relation is
    F(\beta) = U(\beta) - \frac{1}{\beta} S(\beta)
where the log-partition function (free energy) is defined as F(\beta) = -\beta^{-1} \log Z(\beta), the internal energy is U(\beta) = \partial_\beta (\beta F(\beta)) and S(\beta) = \beta^2 \partial_\beta (F(\beta)) is the classical entropy.

Note that the above equation is about the entire Gibbs distribution, in our work we define a "local free energy" F(x, \beta) via the modified Gibbs distribution in Eqn. 3. We can add this discussion to the paper.

[CFG14] Chen, Tianqi, Emily B. Fox, and Carlos Guestrin. "Stochastic Gradient Hamiltonian Monte Carlo." ICML. 2014.
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient mcmc and stochastic optimization." arXiv:1512.07962 (2015).


----------------------------------------------------------------------

Thanks for your comments, allow us to elaborate.

Let \Xi denote the dataset, z denote the weights and x be the parameters of the variational distribution on weights, i.e., the weights are sampled from the distribution q_x(z) := p_x(z | \Xi). Define the loss as the log-likehood of data, viz. -f(z) := \log p(\Xi | z). The ELBO is then given by
(1) \log p(\Xi)
        \geq E_{z \sim q_x(z)} [\log p(\Xi | z)] - KL(q_x(z) || p(z))
        = E_{z \sim q_x(z)} [-f(z)] - KL(q_x(z) || p(z))
and we typically maximize the right hand side wrt x. Let us emphasize that in variational inference, x are the parameters and z are random variables. Also p(z) is a fixed (parameter-free) prior on the weights z.

As noted in our previous comment, while our approach is not variational in nature, we shall now aim to connect the ELBO with local entropy. Focus on the local entropy term defined in Eqn. 4 in the paper: we have
(2) \log F(x,\gamma)
        = \log \int_{z \in Z} e^{-f(z; \Xi) - \gamma/2 |x-z|^2} dz
        \geq \int_{z \in Z} [-f(z; \Xi) - \gamma/2 |x-z|^2] dz.
Eqns. (1) and (2) are thus very different objects. In general, there does not exist a direct mapping between local entropy and variational inference.

We can however provide the following connected for a special case. Consider the family of flat variational distributions q_x(z) such that the distribution is uniform on some compact set around x, e.g., q_x(z) \propto c if |x-z| < C and zero otherwise. We will pick a special non-fixed prior p(z) \propto \exp(-\gamma/2 |x-z|^2). The expectation in Eqn. (1) can now be simplified as:
(3) \log p(\Xi)
        \geq \int_{z \in Z} [-f(z)] - KL(q_x(z) || p(z))
        = -\int_{z \in Z} c*f(z) dz + H(q_x(z)) - \int_{z \in Z} c*\gamma/2 |x-z|^2 dz;
The differential entropy entropy H(q_x(z)) is a constant wrt x. The other two terms are the same as Eqn. (2) (multiplied by a constant c).

In other words "local entropy is similar to performing variational inference within a special class of flat distributions around the paramter x". Such an explanation also validates our claim to finding flat regions in the energy landscape. The above mapping is however only heuristic because the prior p(z) is not fixed and depends on x itself which is unusual. The gradient of local entropy clarifies this:
    dF  = -\gamma (x - E_{x' \sim r(z; x)} [z]);
where the distribution r(z; x) is
    \log r(z; x) \propto \log p(\Xi | z)  - \gamma/2 |x-z|^2
and is thus a likelihood and prior that "moves" along with current iteratre x itself.

It is thus easy to see the differences between a variational approach on the weights and our method. At present we do not see a more rigorous way to reduce our method to SVI, but we would welcome specific suggestions if that is indeed the case.

We have tested SGLD for LeNet on MNIST and All-CNN-BN on CIFAR and can add the results to the paper. In summary, after a hyper-parameter search, the best results we obtained yielded a test error of LeNet of 0.63 \pm 0.1% on MNIST after 300 epochs and 9.89 \pm 0.11% on All-CNN-BN after 500 epochs. Disregarding the slow convergence of SGLD, its generalization error is worse than the results in our paper, viz. 0.48% with Entropy-SGD on LeNet (0.51% with SGD) and 8.65% with Entropy-SGD on All-CNN-BN (8.3% with SGD). For comparison, the authors in [CCF15] report an error of 0.71% with SGLD on MNIST with a slightly larger network (and get 0.47% error with Santa). To our knowledge, there are no results in the literature where MCMC methods perform comparably to SGD on larger networks.

-------------------------------------------------------------------------

You claim that "For modern deep networks with a few million weights, computing the Hessian-vector product as done in HS97 is prohibitive".  But in general the HVP can be computed for only twice the time cost as a gradient evaluation.  Can you elaborate on why this is expensive in your case?

The above comment is for HVP of a general deep network, not our algorithm. And indeed, you are right that HVP can be computed using forward differencing with two back-props [P94, CSP93]. However, as [P94] mentions, it is a

[CSP93] LeCun, Yann, Patrice Y. Simard, and Barak Pearlmutter. "Automatic learning rate maximization by on-line estimation of the Hessianâ€™s eigenvectors." NIPS 1993.
[P94] Pearlmutter, Barak A. "Fast exact multiplication by the Hessian." Neural computation 6.1 (1994): 147-160.


- Bregman vs. KL
- On the use of stochastic hessian information in optimization methods for machine learning
- https://goo.gl/qxWzgr
