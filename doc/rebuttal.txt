AC rebuttal
==========

We are happy to provide clarifications to the questions posed. We enclose them below.

"mostly reinventing SVI"
SVI computes the posterior distribution of hidden variables given data by maximizing the ELBO. Please note that we do not have hidden variables in our formulation; "x" are the variables of our objective f(x), not data. We are interested in maximizing the free energy F(x,\gamma) as defined in Eqn. 4. This is the *free energy of the energy landscape* (defined via a Gibbs distribution). This is thus unrelated to marginal likelihood or variational ELBO where one can impose entropic constraints on the posterior distribution of the *hidden variables* via a prior.

"I suspect it'd become clear that you are maximizing the marginal likelihood"
Please note that we are not maximizing the marginal likelihood, as discussed above.

"how is this better than SGLD", "bizarre that you compare against Adam instead of SGLD"
SGLD is an MCMC algorithm that draws samples from a given distribution. If the step-size goes to zero slowly enough, akin to all MCMC algorithms, it converges to the maximum of the likelihood in exponentially long time-scales (see algorithms like SGHMC [CFG14], Santa [CCG15] etc. that train Bayesian neural networks using MCMC algorithms). Note that SGLD does not optimize the marginal likelihood because there is no notion of hidden variables in vanilla SGLD. We provide a brief review of SGLD in Appendix A of our paper.

We do not know of any results in the literature that train large deep networks such as the one used for CIFAR to get competitive error rates using SGLD. We would like to emphasize that Entropy-SGD simply uses MCMC sampling to estimate Eqn. 7 but it is unrelated to SGLD otherwise.

On the other hand, Adam is an algorithm for computing the maximum of a the likelihood of data given parameters or (equivalently, minimize the loss function). Entropy-SGD is an algorithm designed for minimizing the loss function f(x), in particular, it is not an MCMC algorithm that draws samples from the likelihood. It however does not explicitly do so and instead maximizes the local entropy. We therefore compare Entropy-SGD with state-of-the-art algorithms for training deep networks like Adam and SGD.

"frustrating that you discuss free energy and entropy without precise definitions"
Local entropy (local free energy) is formally defined in Def. 1 (Eqn. 4) but it is already introduced on pg. 2 in the Introduction. The discussion towards the end of Sec. 3 (pg. 5, first line) defines the classical entropy of a Gibbs distribution. The beginning of Sec. 3 based on Fig. 2 is intended to explain things to the reader at an intuitive level before proceeding to the formal definitions which are nevertheless present.

Free energy and classical entropy are related by the informal description "free energy = internal energy - temperature x entropy". Formally, the relation is
    F(\beta) = U(\beta) - \frac{1}{\beta} S(\beta)
where the log-partition function (free energy) is defined as F(\beta) = -\beta^{-1} \log Z(\beta), the internal energy is U(\beta) = \partial_\beta (\beta F(\beta)) and S(\beta) = \beta^2 \partial_\beta (F(\beta)) is the classical entropy.

Note that the above equation is about the entire Gibbs distribution, in our work we define a "local free energy" F(x, \beta) via the modified Gibbs distribution in Eqn. 3. We can add this discussion to the paper.

[CFG14] Chen, Tianqi, Emily B. Fox, and Carlos Guestrin. "Stochastic Gradient Hamiltonian Monte Carlo." ICML. 2014.
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient mcmc and stochastic optimization." arXiv:1512.07962 (2015).


----------------------------------------------------------------------

Thanks for your comments, allow us to elaborate further.

In the notation of the our paper let x denote the parameters and \xi_k denote individual samples. The ELBO is then given by
    L = E_{x \sim q(x)} [\log p(\Xi | x)] - KL(q(x) || p(x))
where p(x) is a fixed prior on the parameters and q(x) is the posterior distribution q(x | \Xi). In mean-field variational inference, we make a mean-field approximation of the distribution q(x) and have to come up with a prior p(x) (see [GG15,BKM16]). Using a Monte-Carlo approximation, the gradient of ELBO can be given by
    dL = \sum_{k=1}^N \log p(\xi | x_k) - KL(q(x) || p(x))
for samples x_k \sim q(x). The first term is simply how well x fits the data while the second term is the distance of the posterior from the prior. If p(x) = N(0, I), one can evaluate the KL term in closed form if q(x) is also a product of Gaussians etc. and compute dL. Note that this approach requires a number of critical assumptions such as a factorizing of q(x) for ease of sampling in the Monte-Carlo term and evaluating the KL term, Gaussian / some other easy to compute prior on weights. These assumptions are not typically valid in CNNs.

The gradient in Eqn. 7 in our paper can be written as:
    dF = -\gamma (x - E_{x' \sim r(x'; x, \xi_k)} [x'])
with the corresponding Monte-Carlo estimate of the expectation. The crucial difference is that the distribution r(x'; x) in our case is the one described in Eqn. 3, i.e.,
    \log r(x'; x) = \log q(x')  + \gamma |x-x'|^2/2 - \log Z_{x,\gamma}.
q(x') can be the same distribution as variational inference above. However, note that r(x'; x) in our work is a function of the current iterate x itself. This is akin to a "prior" that moves with the iterates x which is very different from a Bayesian neural network with a fixed prior.

It is thus easy to see the differences between a variational approach on the weights and our method. We agree that the motivation for our work and approximate inference methods is similar but our method is much different from variational approximation. We'd be very interested if the reviewer has other ways of connecting our paper with it.

"SGLD is a simpler method"
We respectfully disagree. In practice, it is very hard to get SGLD to work as well as SGD or Adam. This has motivated numerous other MCMC algorithms such as SGHMC, SGNHT [DFB14], Santa [CCF15] etc. None of these have yet been shown to beat SGD's error rates for large deep networks.

While it is not hard to add experiments comparing Entropy-SGD to SGLD, this would be a digression. The error rates for baselines in our paper are close to the best error rates reported in the literature for these networks, e.g., on CIFAR, we get 8.3% with SGD and 8.65% with Entropy-SGD both of which are better than those of the original authors (9.08%) [SDB15]. The authors in [CCF15] get a comparable error rate on MNIST (0.47% vs 0.48% for us) with a larger network (Table 1) and a much more complex algorithm. [CCF15] also provides the results for SGLD on LeNet which are much worse than ours. Therefore, we feel our comparisons against baselines are entirely fair.

[GG15] Gal, Yarin, and Zoubin Ghahramani. "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning." arXiv:1506.02142 (2015).
[BKM16] Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. "Variational inference: A review for statisticians." arXiv:1601.00670 (2016).
[SDB15] Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." arXiv:1412.6806 (2014).
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient MCMC and stochastic optimization." arXiv:1512.07962 (2015).
[DFB14] Ding, Nan, et al. "Bayesian sampling using stochastic gradient thermostats." NIPS. 2014.