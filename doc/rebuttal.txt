AC rebuttal
==========

We are happy to provide clarifications to the questions posed. We enclose them below.

"mostly reinventing SVI"
SVI computes the posterior distribution of hidden variables given data by maximizing the ELBO. Please note that we do not have hidden variables in our formulation; "x" are the variables of our objective f(x), not data. We are interested in maximizing the free energy F(x,\gamma) as defined in Eqn. 4. This is the *free energy of the energy landscape* (defined via a Gibbs distribution). This is thus unrelated to marginal likelihood or variational ELBO where one can impose entropic constraints on the posterior distribution of the *hidden variables* via a prior.

"I suspect it'd become clear that you are maximizing the marginal likelihood"
Please note that we are not maximizing the marginal likelihood, as discussed above.

"how is this better than SGLD", "bizarre that you compare against Adam instead of SGLD"
SGLD is an MCMC algorithm that draws samples from a given distribution. If the step-size goes to zero slowly enough, akin to all MCMC algorithms, it converges to the maximum of the likelihood in exponentially long time-scales (see algorithms like SGHMC [CFG14], Santa [CCG15] etc. that train Bayesian neural networks using MCMC algorithms). Note that SGLD does not optimize the marginal likelihood because there is no notion of hidden variables in vanilla SGLD. We provide a brief review of SGLD in Appendix A of our paper.

We do not know of any results in the literature that train large deep networks such as the one used for CIFAR to get competitive error rates using SGLD. We would like to emphasize that Entropy-SGD simply uses MCMC sampling to estimate Eqn. 7 but it is unrelated to SGLD otherwise.

On the other hand, Adam is an algorithm for computing the maximum of a the likelihood of data given parameters or (equivalently, minimize the loss function). Entropy-SGD is an algorithm designed for minimizing the loss function f(x), in particular, it is not an MCMC algorithm that draws samples from the likelihood. It however does not explicitly do so and instead maximizes the local entropy. We therefore compare Entropy-SGD with state-of-the-art algorithms for training deep networks like Adam and SGD.

"frustrating that you discuss free energy and entropy without precise definitions"
Local entropy (local free energy) is formally defined in Def. 1 (Eqn. 4) but it is already introduced on pg. 2 in the Introduction. The discussion towards the end of Sec. 3 (pg. 5, first line) defines the classical entropy of a Gibbs distribution. The beginning of Sec. 3 based on Fig. 2 is intended to explain things to the reader at an intuitive level before proceeding to the formal definitions which are nevertheless present.

Free energy and classical entropy are related by the informal description "free energy = internal energy - temperature x entropy". Formally, the relation is
    F(\beta) = U(\beta) - \frac{1}{\beta} S(\beta)
where the log-partition function (free energy) is defined as F(\beta) = -\beta^{-1} \log Z(\beta), the internal energy is U(\beta) = \partial_\beta (\beta F(\beta)) and S(\beta) = \beta^2 \partial_\beta (F(\beta)) is the classical entropy.

Note that the above equation is about the entire Gibbs distribution, in our work we define a "local free energy" F(x, \beta) via the modified Gibbs distribution in Eqn. 3. We can add this discussion to the paper.

[CFG14] Chen, Tianqi, Emily B. Fox, and Carlos Guestrin. "Stochastic Gradient Hamiltonian Monte Carlo." ICML. 2014.
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient mcmc and stochastic optimization." arXiv:1512.07962 (2015).


----------------------------------------------------------------------

Title: SVI does not resemble Entropy-SGD without a "moving prior", a feature of our scheme

To address the objections 1) and 3) let \Xi denote the dataset, z denote the weights and x be the parameters of a variational distribution q_x(z). The ELBO can then be written as
(i) \log p(\Xi)
        \geq E_{z \sim q_x(z)} [\log p(\Xi | z)] - KL(q_x(z) || p(z))
and maximized with respect to x. The distribution p(z) is a fixed (parameter-free) prior, which one has to postulate.

On the other hand, Eqn. 4 in the paper can be used to write the log of local entropy as:
(ii) \log F(x,\gamma)
        = \log \int_{z \in Z} e^{-f(z; \Xi) - \gamma/2 |x-z|^2} dz
        \geq \int_{z \in Z} [-f(z; \Xi) - \gamma/2 |x-z|^2] dz;
where f(z) = -\log p(\Xi | z). We are unaware of a general way to choose a prior p(z) and a variational family q_x(z) that that makes (i) resemble (ii), and therefore interpret our method as “integrating out […] the posterior over neural network parameters.” The only way we can do so is to pick a specific “prior" that depends on the parameter x (hence, not really a prior). For instance, one could choose a uniform variational family (say, q_x(z) \propto constant for |x-z| \leq C and zero otherwise) and a Gaussian “prior" with mean x (\log p(z) = -\gamma/2 |x-z|^2) to make (ii) resemble ELBO. In this case p(z) would not be fixed, but it would “move” along with the current iterate x.

This "moving prior" is a crucial feature of our proposed algorithm. The gradient of local entropy (Eqn. 7 in the paper) clarifies this further:
    dF  = -\gamma (x - E_{z \sim r(z; x)} [z]);
where the distribution r(z; x) is
    r(z; x) \propto p(\Xi | z) \exp(-\gamma/2 |x-z|^2);
i.e. it contains a data likelihood term with a prior that "moves" along with the current iterate x.

Concerning the relation to SGLD, consider Belief Propagation (BP). Our proposed algorithm relates to the "focusing-Belief Propagation" variant (fBP), rather than the standard one [BBC16]. The difference between BP and fBP is analogous to that between SGLD and Entropy-SGD: the latter operates on a transformation of the energy landscape of the former, exploiting local entropic effects. This difference is crucial and indeed related to the "moving prior" of the previous discussion; plain SGLD (or BP) can only trade energy for entropy via the temperature parameter which does not allow for direct use of any geometric information of the landscape and does not help with narrow minima.

In view of your comment 2), we also implemented SGLD for LeNet on MNIST and All-CNN-BN on CIFAR and will add the following results to our paper: After a hyper-parameter search, the best we obtained were a test error of LeNet on 0.63 \pm 0.1% on MNIST after 300 epochs and 9.89 \pm 0.11% on All-CNN-BN after 500 epochs. Disregarding the slow convergence of SGLD, its generalization error is slightly worse than the results in our paper, viz. 0.48% with Entropy-SGD on LeNet (0.51% with SGD) and 8.65% with Entropy-SGD on All-CNN-BN (8.3% with SGD). For comparison, the authors in [CCF15] report an error of 0.71% with SGLD on MNIST with a slightly larger network (0.47% with Santa), there are no results in the literature where MCMC methods perform comparably to SGD on larger networks.

[BBC16] Baldassi, Carlo, et al. "Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes." PNAS (2016).
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient MCMC and stochastic optimization." arXiv:1512.07962 (2015).

-------------------------------------------------------------------------

Title: Re: Is the Hessian-vector product actually expensive?

Comment 3) above is for HVP of the loss function of a general deep network, not local entropy. Indeed, HVPs can be computed using forward differencing with two back-props. Such an approximation is susceptible to numerical errors --- especially in high dimensions [P94, M10]. One typically averages the HVP over many samples in the dataset which is expensive, for instance, the authors in [LSP93] average over a few hundred samples, which roughly translates to 5-10x the time required for one iteration of vanilla SGD. More accurate algorithms like that of [P94] also require this averaging over samples.

One could argue that accuracy in the approximation of HVP does not matter in practice for purposes of training; what matters is only the local curvature at a scale commensurate with the typical weight updates. The perturbation vector for computing HVPs using back-props thus needs to be chosen carefully. If so, indeed, approximate computation of HVP can be considered cheap.

[LSP93] LeCun, Yann, Patrice Y. Simard, and Barak Pearlmutter. "Automatic learning rate maximization by on-line estimation of the Hessian’s eigenvectors." NIPS (1993).
[P94] Pearlmutter, Barak A. "Fast exact multiplication by the Hessian." Neural computation 6.1 (1994): 147-160.
[M10] Martens, James. "Deep learning via Hessian-free optimization." ICML (2010).


-----------------------------------------------------------------------



REBUTTAL
 
We thank the Reviewers and others for their valuable feedback. We incorporated all specific comments into our current draft. We respond below to all reviewers with a general comment (all reviews and comments are also individually addressed by us):
a) Smoothing the original loss function vs. local entropy
- smoothing does not bias towards wide minima (e.g., many sharp minima that are close by will be unaffected), it might reduce the ruggedness of the landscape
- Refer to Fig. 1, the local-entropy landscape is indeed smoother, but more importantly, it has a global minimum in a wide valley.
- smoothing = local entropy only for a large \gamma

b) [comment about the speed]

c) eigenvalues
[fix] Our generalization bound in Thm. 3 relies upon an assumption that requires the Hessian to not have eigenvalues in the set [-2 \gamma-c, c], where the constant c depends on both the dataset and the architecture; this is unfortunately quite unnatural. Our argument is indeed only a sketch and we will clarify this. It also uses Laplace's method to show that the modified loss function is smoother; this is valid only for large values of \gamma. We would however like to point out that one requires the bound to hold only during the optimization and not everywhere in the landscape (as Fig. 1 shows, this assumption is not true at the ~local minima themselves). A rigorous proof of such a statement however requires an analysis of the dynamics of SGD and is currently out of reach.


------------------
AnonReviewer4:

(1) The confusion about the two claims is probably caused by the wording of our remark, which is a statement about the energy landscape based on our experimental results. We always get an equivalent or better generalization error using Entropy-SGD and a lower cross-entropy loss on the _training set_ (Fig. 5a). This suggests that there are wide local minima at lower energy levels in the energy landscape which Entropy-SGD manages to find.

(2) Please see the updates above for additional experiments that demonstrate acceleration in wall-clock time with respect to SGD.

(3) Please see the latest updates to the paper. We can now train all our networks with only the local entropy loss, i.e., rho = 0, using scoping on the parameter \gamma.

(4) [fix] We have expanded the discussion on [HS97] in Sec. 2 to dwell upon the a few similarities. We agree that the Gibbs formalism to prove improved generalization of flat minima is a promising direction for further analysis. The Gibbs variant to averaging in weight space (Eqn. 33, pg. 22 of [HS97]) is similar to the averaging in Eqn. 7 of our paper.

------------------
Csaba Szepesvari:

__Related work__: We have expanded the discussion regarding [Baldassi et. al. '16] in Sec. 2 and Sec. 3.

------------------
AnonReviewer1:

Thanks, it should indeed be f(x') in Eqn. 8. 

We have significantly expanded the experimental section and also added an experiment that trains an LSTM on the Penn Tree Bank corpus. Please see updates to the paper for more details.

------------------
AnonReviewer2:

4. We set c from Thm. 3.12 of Hardt's paper to be 1 for clarity (c is the initial learning rate). Including it in the exponent does not change our analysis.

------------------
Areachair1 [Stefano should maybe edit and post that comment]:

Re: You should make these connections clearer and more explicit
We have discussed this in depth with sufficient technical details above. Using terms like "similar to motivations of Bayesian methods" and "using Bayesian methods in the inner loop" is generalizing to the point of absurdity.

It is not accurate to term local entropy as "SVI with a moving prior" for a number of reasons. "SVI with a moving prior" has no theoretical basis. It is not connected to the log-likelihood of data, it is connected to local entropy instead. As we have discussed above, local entropy *resembles* SVI with a moving prior but only in an extremely non-rigorous and merely conceptual fashion.

We have updated the experiments in our paper. We now plot the number of gradient evaluations on the x-axis and using scoping, we can in fact show that local entropy performs better in a smaller amount of time. As requested, we have also added a section comparing local-entropy with SGLD.

Re: Exact Hessian-vector product is easy to compute
We have discussed this above (see the comment regarding [P94]). Computing the "exact" HVP requires averaging over the entire dataset, i.e., ~4x the complexity of one epoch, which is expensive.
