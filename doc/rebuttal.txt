"mostly reinventing SVI"
SVI computes the posterior distribution of hidden variables given data by maximizing the ELBO. We do not have hidden variables in our formulation. Note that variable "x" are the variables of our objective f(x), not data. We are interested in maximizing the free energy F(x,\gamma) (related to the entropy of the optimization objective f(x)) as defined in Eqn. 4. This is totally unrelated to marginal likelihood or variational ELBO.

"I suspect it'd become clear that you are maximizing the marginal likelihood"
We are not maximizing the marginal likelihood.

"how is this better than SGLD", "bizzare that you compare against Adam"
SGLD is an MCMC algorithm that draws samples from a given distribution. If the step-size goes to zero slowly enough, akin to all MCMC algorithms, it converges to the maximum of the likelihood in exponentially long time-scales (see algorithms like SGHMC [CFG14], Santa [CCG15] etc. that train Bayesian neural networks using MCMC algorithms). Note, this is not the marginal likelihood because there is no notion of hidden variables in SGLD. Please see Appendix A for more details.

On the other hand, Adam is an algorithm for computing the maxmimum of a the likelihood of data given parameters or (equivalently, minimize the loss function). Entropy-SGD is desgined for minimizing the loss function f(x). It however does not explicitly do so and instead maximizes the local entropy. We therefore compare the two.

Local entropy (local free energy) is defined in Def. 1 (Eqn. 4) while the discussion (pg. 5, first line) defines the classical entropy of a Gibbs distribution. The beginning of Sec. 3 based on Fig. 2 is intended to explain things to the read at an intuitive level.

[CFG14] Chen, Tianqi, Emily B. Fox, and Carlos Guestrin. "Stochastic Gradient Hamiltonian Monte Carlo." ICML. 2014.
[CCF15] Chen, Changyou, et al. "Bridging the gap between stochastic gradient mcmc and stochastic optimization." arXiv:1512.07962 (2015).