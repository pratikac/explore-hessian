\pdfoutput = 1

\documentclass[10pt]{article}
\input{macros}
\usepackage{iclr2017_conference}
\usepackage{mathptmx}

\linespread{1.02}

%\pagestyle{empty}

% save space
\setlength{\floatsep}{0.0in}
\setlength{\textfloatsep}{0.0in}
\setlength{\intextsep}{0.05in}
\setlength{\belowcaptionskip}{0.05in}
\setlength{\abovecaptionskip}{0.05in}
\setlength{\abovedisplayskip}{0.05in}
\setlength{\belowdisplayskip}{0.05in}

% notation goes here
\newcommand\red[1]{{\color{red}#1}}
\newcommand{\entropysgd}{\mathrm{Entropy}\textrm{-}\mathrm{SGD}}
\newcommand{\entropyadam}{\mathrm{Entropy}\textrm{-}\mathrm{Adam}}
\newcommand{\minibatch}[1]{\Xi^{#1}}
\newcommand{\mnistfc}{\textrm{mnistfc}}
\newcommand{\smallmnistfc}{\textrm{small-mnistfc}}
\newcommand{\charlstm}{\textrm{char-LSTM}}
\newcommand{\ptblstm}{\textrm{PTB-LSTM}}
\newcommand{\lenet}{\textrm{LeNet}}
\newcommand{\allcnn}{\textrm{All-CNN-BN}}

\title{Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}

\author{Pratik Chaudhari$^{1}$, Anna Choromanska$^{2}$, Stefano Soatto$^{1}$, Yann LeCun$^{2,3}$, Carlo Baldassi$^{4}$,\\[0.03in]
\textbf{Christian Borgs$^{5}$, Jennifer Chayes$^{5}$, 
Riccardo Zecchina$^{4}$}\\[0.05in]
$^{1}$ Computer Science, University of California, Los Angeles.\\
$^{2}$ Courant Institute of Mathematical Sciences, New York University.\\
$^{3}$ Facebook AI Research, New York.\\
$^{4}$ Politecnico di Torino.\\
$^{5}$ Microsoft Research New England, Cambridge.\\ [0.05in]
{\footnotesize
Email:\ \href{mailto:pratikac@ucla.edu}{pratikac@ucla.edu},
\href{mailto:achoroma@cims.nyu.edu}{achoroma@cims.nyu.edu},
\href{mailto:soatto@cs.ucla.edu}{soatto@cs.ucla.edu},
\href{mailto:yann@cs.nyu.edu}{yann@cs.nyu.edu},
\href{mailto:carlobaldassi@gmail.com}{carlobaldassi@gmail.com}}\\[0.03in]
{\footnotesize
\hspace{0.33in} \href{mailto:borgs@microsoft.com}{borgs@microsoft.com},
\href{mailto:jchayes@microsoft.com}{jchayes@microsoft.com},
\href{mailto:riccardo.zecchina@polito.it}{riccardo.zecchina@polito.it}
}}

% macros to color text and insert margin comments
\setlength{\marginparwidth}{1in}
\newcommand{\pc}[2]{{\color{ForestGreen}#1}\marginpar{\tiny\noindent{\raggedright{\color{Sienna}[PC]}\color{Sienna}{#2} \par}}}
\renewcommand{\ss}[2]{{\color{cyan}#1}\marginpar{\tiny\noindent{\raggedright{\color{BurntOrange}[SS]}\color{BurntOrange}{#2} \par}}}
\newcommand{\ac}[2]{{\color{magenta}#1}\marginpar{\tiny\noindent{\raggedright{\color{magenta}[AC]}\color{magenta}{#2} \par}}}
\newcommand{\todo}[1]{{\color{gray}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[TODO]}}}}
\newcommand{\fix}[2]{{\color{blue}#1}\marginpar{\tiny\noindent{\raggedright{\color{blue}[FIX]}\color{blue}{#2} \par}}}
\newcommand{\ignore}[1]{}

% uncomment these for submission!
% \renewcommand{\pc}[2]{#1}
% \renewcommand{\ss}[2]{#1}
% \renewcommand{\ac}[2]{#1}
% \renewcommand{\todo}[1]{#1}

\graphicspath{{./fig/}}

%\iclrfinalcopy % Uncomment for camera-ready version
\begin{document}

\maketitle

\begin{abstract}
This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local entropy based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. \fix{We prove that the local entropy objective is smoother than the original landscape and show improved generalization bounds over SGD using uniform stability.}{smooth} Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably with state-of-the-art techniques in terms of generalization performance and training time.
\end{abstract}

\section{Introduction}
\label{s:intro}
This paper presents a new optimization tool for deep learning designed to exploit the local geometric properties of the objective function. Consider the histogram in Fig.~\ref{fig:lenet_hessian} showing the spectrum of the Hessian at an extremum discovered by Adam~\citep{kingma2014adam} for a convolutional neural network on MNIST~\citep{lecun1998gradient} with about $47,000$ weights (cf.\@ Sec.~\ref{ss:expt:universality}). It is evident that:
\begin{enumerate}[(i)]
\item a large number of directions ($\approx 94\%$) have near-zero eigenvalues (magnitude less than $10^{-4}$),
\item positive eigenvalues (right inset) have a long tail with the largest one being almost $40$,
\item negative eigenvalues (left inset), which are directions of descent that the optimizer missed, have a much faster decay (the largest negative eigenvalue is only $-0.4$).
\end{enumerate}
Interestingly, this trend is not unique to this particular network. Rather, its qualitative properties are shared across a variety of network architectures, sizes, datasets or optimization algorithms (refer to Sec.~\ref{s:expt} for more experiments). Local minima that generalize well and are discovered by gradient descent lie in ``wide valleys'' of the energy landscape, rather than in sharp isolated minima. For an intuitive understanding of this phenomenon, imagine a Bayesian prior concentrated at the minimizer of the expected loss, the marginal likelihood of wide valleys under this prior is much higher than narrow, sharp valleys even if the latter are close to the global minimum in training loss. Almost-flat regions of the energy landscape are robust to data perturbations, noise in the activations, as well as perturbations of the parameters --- all of which are widely-used techniques to achieve good generalization. This suggests that wide valleys should result in better generalization and, indeed, standard optimization algorithms in deep learning --- without being explicitly tailored to do so --- seem to discover exactly that.

\begin{figure}[tbh]
% \vspace*{-0.1in}
\centering
\includegraphics[width=\textwidth]{lenet_hessian.pdf}
\caption{\small Eigenspectrum of the Hessian at a local minimum of a CNN on MNIST (two independent runs). \textbf{Remark:} The central plot shows the eigenvalues in a small neighborhood of zero whereas the left and right insets show the entire tails of the eigenspectrum.}
\label{fig:lenet_hessian}
\end{figure}

Based on this understanding of how the local geometry looks at the end of optimization, can we modify SGD to actively seek such regions? Motivated by the work of~\citet{baldassi2015subdominant} on shallow networks, instead of minimizing the original loss $f(x)$, we propose to maximize
$$
F(x, \g) = \log \int_{x' \in \reals^n}\ \exp \rbrac{-f(x') - \f{\g}{2}\ \norm{x-x'}^2_2}\ dx'.
$$
The above is a log-partition function that measures both the depth of a valley at a location  $x \in \reals^n$, and its flatness through the entropy of $f(x')$; we call it ``local entropy'' in analogy to the free entropy used in statistical physics. The $\entropysgd$ algorithm presented in this paper employs stochastic gradient Langevin dynamics (SGLD) to approximate the gradient of local entropy. Our algorithm resembles two nested loops of SGD: the inner loop consists of SGLD iterations while the outer loop updates the parameters. We show that the above modified loss function results in a smoother energy landscape defined by the hyper-parameter $\g$ which we can think of as a ``scope'' that seeks out valleys of specific widths. Actively biasing the optimization towards wide valleys in the energy landscape results in better generalization error. We present experimental results on fully-connected and convolutional neural networks (CNNs) on the MNIST and CIFAR-10~\citep{krizhevsky2009learning} datasets and recurrent neural networks (RNNs) on the Penn Tree Bank dataset (PTB)~\citep{marcus1993building} and character-level text prediction. Our experiments show that $\entropysgd$ scales to deep networks used in practice, obtains comparable generalization error as competitive baselines and also trains much more quickly than SGD (we get a $2\trm{x}$ speed-up for RNNs).

\section{Related work}
\label{s:prior_work}

Our above observation about the spectrum of Hessian (further discussed in Sec.~\ref{s:expt}) is similar to results on a perceptron model in~\citet{dauphin2014identifying} where the authors connect the loss function of a deep network to a high-dimensional Gaussian random field. They also relate to earlier studies such as~\citet{Baldi:1989:NNP:70359.70362,Fyodorov2007,Bray2007} which show that critical points with high training error are exponentially likely to be saddle points with many negative directions and all local minima are likely to have error that is very close to that of the global minimum. The authors also argue that convergence of gradient descent is affected by the proliferation of saddle points surrounded by high error plateaus --- as opposed to multiple local minima. One can also see this via an application of Kramer's law: the time spent by diffusion is inversely proportional to the smallest negative eigenvalue of the Hessian at a saddle point~\citep{bovier2006metastability}.

The existence of multiple, almost equivalent, local minima in deep networks has been predicted using a wide variety of theoretical analyses and empirical observations, e.g., papers such as~\citet{spinglass2015,DBLP:conf/colt/ChoromanskaLA15,chaudhari2015trivializing} that build upon results from statistical physics as also others such as~\citet{haeffele2015global} and~\citet{janzamin2015beating} that obtain similar results for matrix and tensor factorization problems. Although assumptions in these works are somewhat unrealistic in the context of deep networks used in practice, similar results are also true for linear networks which afford a more thorough analytical treatment~\citep{DBLP:journals/corr/SaxeMG13}. For instance,~\citet{soudry2016no} show that with mild over-parameterization and dropout-like noise, training error for a neural network with one hidden layer and piece-wise linear activation is zero at every local minimum. All these results suggest that the energy landscape of deep neural networks should be easy to optimize and they more or less hold in practice --- it is easy to optimize a prototypical deep network to near-zero loss \emph{on the training set}~\citep{hardt2015train,DBLP:journals/corr/GoodfellowV14}.
%Works such as which shows that for deep linear networks with more than three layers, there exist ``bad'' saddle points, i.e., where the Hessian has no negative eigenvalues; SGD will find it difficult to make progress in such regions.

Obtaining good \emph{generalization} error, however, is challenging: complex architectures are sensitive to initial conditions and learning rates~\citep{sutskever2013importance} and even linear networks~\citep{kawaguchi2016deep} may have degenerate and hard to escape saddle points~\citep{ge2015escaping,anandkumar2016efficient}. Techniques such as adaptive~\citep{duchi2011adaptive} and annealed learning rates, momentum~\citep{tieleman2012lecture}, as well as architectural modifications like dropout~\citep{srivastava2014dropout}, batch-normalization~\citep{ioffe2015batch,cooijmans2016recurrent}, weight scaling~\citep{salimans2016weight} etc. are different ways of tackling this issue by making the underlying landscape more amenable to first-order algorithms. However, the training process often requires a combination of such techniques and it is unclear beforehand to what extent each one of them helps.

Closer to the subject of this paper are results by~\citet{baldassi2015subdominant,baldassi2016unreasonable,baldassi2016multilevel} who show that the energy landscape of shallow networks with discrete weights is characterized by an exponential number of isolated minima and few very dense regions with lots of local minima close to each other. These dense local minima can be shown to generalize well for random input data; more importantly, they are also accessible by efficient algorithms using a novel measure called ``robust ensemble'' that amplifies the weight of such dense regions. The authors use belief propagation to estimate local entropy for simpler models considered there. A related work in this context is EASGD~\citep{zhang2015deep} which trains multiple deep networks in parallel and modulates the distance of each worker to the ensemble average. Such an ensemble training procedure enables improved generalization by ensuring that different workers land in the same wide valley and indeed, it turns out to be closely related to the replica theoretic analysis of~\citep{baldassi2016unreasonable}.

Our work generalizes the local entropy approach above to modern deep networks with continuous weights. It exploits the same phenomenon of wide valleys in the energy landscape but does so without incurring the hardware and communication complexity of replicated training or being limited to models where one can estimate local entropy using belief propagation. The enabling technique in our case is using Markov chain Monte Carlo for estimating the gradient of local entropy, which can be done efficiently even for large deep networks using mini-batch updates.

The authors in~\citet{hochreiter1997flat} introduce a similar technique as our work, they introduce hard constraints on the width of local minima and training loss and show using the Gibbs formalism~\citep{haussler1997mutual} that this leads to improved generalization. \fix{Their analysis of the generalization performance (see their Sec. A1) necessitates a few unrealistic assumptions whereas in our analysis, as will be shown, we use uniform stability~\citet{bousquet2002stability} to obtain a precise characterization of generalization.}{one sentence about the similarities} Furthermore, as they mention, the effect of hyper-parameters for the constraints they use is intricately tied together and they are difficult to choose even for small problems. Their formulation depends on Hessian-vector products. It is unclear how this method could be scaled to modern deep networks. Motivated by the same final goal, viz., flat local minima, our approach eliminates these extraneous and difficult to control hyper-parameters and instead results in a scalable algorithm. Also recent work by~\citet{keskar2016large} is relevant in the context of the main theme of our paper. The authors estimate the loss in a neighborhood of the weights to argue that small batch size in SGD (i.e., larger gradient noise) generalizes better than large mini-batches and also results in significantly flatter minima.

\todo{In the sense that local entropy results in a smoother energy landscape (cf.\@ Sec.~\ref{ss:theoretical_properties}), it resembles continuation methods~\citep{allgower2012numerical} that convolve the loss function to solve sequentially refined optimization problems. These techniques however require a special class of loss functions where successively smaller convolutions result in minimizers that lie close to each other or closed-form expressions of smoothened loss functions~\citep{DBLP:conf/icml/HazanLS16,mobahi2015link}; these may not be possible for deep networks with isolated minima and highly-nonlinear loss functions. Our modified objective instead weighs each location by the local entropy in its neighborhood and does not require stringent assumptions on the objective, it is also applicable to full-scale deep networks.} %\pc{Let us also note that motivated from continuation,~\citet{gulcehre2016mollifying} insert annealed additive noise in the activation function to approximately convolve the original objective function.}{This paper is not related to our work, it is related to continuation. I don't see any reason to add this sentence}

\section{Local entropy}
\label{s:local_entropy}

We first provide a simple intuition for the concept of local entropy of an energy landscape. Consider a cartoon energy landscape in Fig.~\ref{fig:entropyfig} where the x-axis denotes the configuration space of the parameters. We have constructed two local minima: a shallower although wider one at $x_{\trm{robust}}$ and a very sharp global minimum at $x_{\trm{non-robust}}$. Under a Bayesian prior on the parameters, say a Gaussian of a fixed variance at locations $x_{\trm{robust}}$ and $x_{\trm{non-robust}}$ respectively, the wider local minimum has a higher marginalized likelihood than the sharp valley on the right.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\includegraphics[width=0.4 \textwidth]{entropyfig.pdf}
\caption{\small Local entropy concentrates on wide valleys in the energy landscape.}
\label{fig:entropyfig}
\vspace*{-0.1in}
\end{wrapfigure}

The above discussion suggests that parameters that lie in wider local minima like $x_{\trm{robust}}$, which may possibly have a higher loss than the global minimum, should generalize better than the ones that are simply at the global minimum. In a neighborhood of $x_{\trm{robust}}$, ``local entropy'' as introduced in Sec.~\ref{s:intro} is large because it includes the contributions from a large region of good parameters; conversely, near $x_{\trm{non-robust}}$, there are fewer such contributions and the resulting local entropy is low. The local entropy thus provides a way of picking large, approximately flat, regions of the landscape over sharp, narrow valleys in spite of the latter possibly having a lower loss. Quite conveniently, the local entropy is also computed from the partition function with a local re-weighting term.

Formally, for a parameter vector $x \in \reals^n$, consider a Gibbs distribution corresponding to a given energy landscape $f(x)$:
\begin{equation}
    \P(x;\ \b) = Z_\b^{-1}\ \exp\ \rbrac{-\b\ f(x)};
    \label{eq:gibbs_original}
\end{equation}
where $\b$ is known as the inverse temperature and $Z_\b$ is a normalizing constant, also known 
as the partition function. As $\b \to \infty$, the probability distribution above concentrates on the global minimum of $f(x)$ (assuming it is unique) given as:
\begin{equation}
    x^* = \argmin_x\ f(x),
    \label{eq:argmin_generic}
\end{equation}
which establishes the link between the Gibbs distribution and a generic optimization problem~\eqref{eq:argmin_generic}. We would instead like the probability distribution --- and therefore the underlying optimization problem --- to focus on flat regions such as $x_{\textrm{robust}}$ in Fig.~\ref{fig:entropyfig}. With this in mind, let us construct a modified Gibbs distribution:
\begin{equation}
    \P(x';\ x, \b, \g) = Z_{x,\b,\ \g}^{-1}\ \exp\ \rbrac{-\b\ f(x') - \b\ \f{\g}{2}\ \norm{x-x'}^2_2}.
    \label{eq:gibbs_modified}
\end{equation}
The distribution in~\eqref{eq:gibbs_modified} is a function of a dummy variable $x'$ and is parameterized by the original location $x$. The parameter $\g$ biases the modified distribution~\eqref{eq:gibbs_modified} towards $x$; a large $\g$ results in a $P(x';\ x,\b,\g)$ with all its mass near $x$ irrespective of the energy term $f(x')$. For small values of $\g$, the term $f(x')$ in the exponent dominates and the modified distribution is similar to the original Gibbs distribution in~\eqref{eq:gibbs_original}. We will set the inverse temperature $\b$ to $1$ because $\g$ affords us similar control on the Gibbs distribution.

\begin{definition}[Local entropy]
\label{def:local_entropy}
The local free entropy of the Gibbs distribution in~\eqref{eq:gibbs_original}, colloquially called ``local entropy'' in the sequel and denoted by $F(x, \g)$, is defined as the log-partition function of modified Gibbs distribution~\eqref{eq:gibbs_modified}, i.e.,
\begin{align}
    F(x, \g) &= \log Z_{x, 1, \g} \notag\\
    &= \log \int_{x'}\ \exp \Big(-f(x') - \f{\g}{2}\ \norm{x-x'}^2_2 \Big)\ dx'.
    \label{eq:local_entropy}
\end{align}
\end{definition}
The parameter $\g$ is used to focus the modified Gibbs distribution upon a local neighborhood of $x$ and we call it a ``scope'' henceforth. Fig.~\ref{fig:entropyfig} shows the negative local entropy $-F(x, \g)$ for the original energy landscape. Note that $-F(x, \g)$ has a global minimum near $x_{\textrm{robust}}$ which is exactly what we want; indeed, $x_{\textrm{robust}}$ has a higher local entropy than $x_{\textrm{non-robust}}$.

\paragraph{Connection to classical entropy:}
The quantity we have defined as local entropy in Def.~\ref{def:local_entropy} is different from classical entropy which counts the number of likely configurations under a given distribution. For a continuous parameter space, this is given by
$$
    S(x, \b, \g) = - \int_{x'}\ \log \P(x';\ x, \b, \g)\ d \P(x';\ x, \b, \g)
$$
for the Gibbs distribution in~\eqref{eq:gibbs_modified}. Minimizing classical entropy however does not differentiate between flat regions that have very high loss versus dense regions that lie deeper in the energy landscape. For instance, in Fig.~\ref{fig:entropyfig}, classical entropy is smallest in the neighborhood of $x_{\textrm{candidate}}$ which is a large region with very high loss on the training dataset and is unlikely to generalize well.
\ignore{We could minimize a modified loss function of the form $f(x) + \lambda S(x, \g)$ whose gradient can be computed to be
$$
    \nabla \Big(f(x) + \lambda S(x, \g)\Big) = \nabla f(x) - \lambda \g\ \corr (g(x'),\ x-x');
$$
where we have defined the cross-correlation as
$$
    \corr (g(x'),\ x-x') := \ag{g(x')\ (x-x')} - \ag{g(x')}\ \ag{x-x'};
$$
with $g(x') = f(x') + \f{\g}{2}\ \norm{x-x'}^2_2$. The gradient of the entropy can again be estimated using Langevin dynamics. Practically, one then has to modulate the hyper-parameter $\lambda$ during the course of the training to first use the gradient $\nabla f(x)$ to make progress and then turn on the entropy term to scope dense clusters.}

\section{Entropy-guided SGD}
\label{s:entropysgd}

We now present the $\entropysgd$ algorithm that is a variation of SGD motivated from local entropy. Simply speaking, it minimizes the negative local entropy from Sec.~\ref{s:local_entropy}. This section discusses how the gradient of local entropy can be computed via Langevin dynamics; the resulting algorithm has a strong flavor of ``SGD-inside-SGD'': the outer SGD updates the parameters, while an inner SGD estimates the gradient of local entropy.

Consider a typical classification setting, let $x \in \reals^n$ be the weights of a deep neural network and $\xi_k \in \Xi$ be samples from a dataset $\Xi$ of size $N$. Let $f(x; \xi_k)$ be the loss function, e.g., cross-entropy of the classifier on a sample $\xi_k$. The original optimization problem is:
\begin{equation}
    x^* = \argmin_x\ \f{1}{N}\ \sum_{k=1}^N\ f(x;\ \xi_k);
    \label{eq:dnn_objective}
\end{equation}
where the objective $f(x, \xi_k)$ is typically, a non-convex function in both the weights $x$ and the sample $\xi_k$. The $\entropysgd$ algorithm instead solves the problem
\begin{equation}
    x^*_{\entropysgd} = \argmin_x\ -F(x, \gamma;\ \Xi);
    \label{eq:entropysgd_objective}
\end{equation}
where we have made the dependence of local entropy $F(x,\g)$ on the dataset $\Xi$ explicit.

\subsection{Gradient of local entropy}
\label{ss:grad_local_entropy}

The gradient of local entropy over a randomly sampled mini-batch of $m$ samples denoted by $\xi_{\ell_i} \in \minibatch{\ell}$ for $i \leq m$ is easy to derive and is given by
\begin{equation}
    -\nabla_x F\rbrac{x, \g;\ \minibatch{\ell}} = \g\ \rbrac{x - \ag{x';\ \minibatch{\ell}}};
    \label{eq:entropysgd_grad}
\end{equation}
where the notation $\ag{\cdot}$ denotes an expectation of its arguments (we have again made the dependence on the data explicit) over a Gibbs distribution of the original optimization problem modified to focus on the neighborhood of $x$; this is given by
\begin{equation}
    \P(x';\ x, \g) \propto\ \exp \sqbrac{-\rbrac{\f{1}{m}\ \sum_{i=1}^m\ f\rbrac{x;\ \xi_{\ell_i}}} - \f{\g}{2}\ \norm{x-x'}_2^2}.
    \label{eq:entropysgd_modified_gibbs}
\end{equation}
Computationally, the gradient in~\eqref{eq:entropysgd_grad} involves estimating $\ag{x';\ \minibatch{\ell}}$ with the current weights fixed to $x$. This is an expectation over a Gibbs distribution and is hard to compute. We can however approximate it using Markov chain Monte-Carlo (MCMC) techniques. In this paper, we use stochastic gradient Langevin dynamics (SGLD)~\citep{welling2011bayesian} that is an MCMC algorithm for drawing samples from a Bayesian posterior and scales to large datasets using mini-batch updates. Please see Sec.~\ref{s:app:langevin} in the Supplement for a brief overview of SGLD. For our application, as lines 3-6 of Alg.~\ref{alg:entropysgd} show, SGLD resembles a few iterations of SGD with additive gradient noise.

We can obtain some intuition of how $\entropysgd$ works using the expression for the gradient: the term $\ag{x';\ \cdot}$ is the average over a locally focused Gibbs distribution and for two local minima in the neighborhood of $x$ roughly equivalent in loss, this term points towards the wider one because $\ag{x';\ \cdot}$ is closer to it. This results in a net gradient that takes SGD towards wider valleys. Moreover, if we unroll the SGLD steps used to compute $\rbrac{x - \ag{x'; \cdot}}$ (cf. line 5 in Alg.~\ref{alg:entropysgd}), it resembles one large step in the direction of the (noisy) average gradient around the current weights $x$ and $\entropysgd$ thus looks similar to averaged SGD in the literature~\citep{polyak1992acceleration,bottou2012stochastic}. These two phenomena intuitively explain the improved generalization performance of $\entropysgd$.

\subsection{Algorithm and Implementation details}
\label{ss:alg}

Alg.~\ref{alg:entropysgd} provides the pseudo-code for one iteration of the $\entropysgd$ algorithm. At each iteration, lines $3$-$6$ perform $L$ iterations of Langevin dynamics to estimate $\mu = \ag{x'; \minibatch{\ell}}$. The weights $x$ are updated with the modified gradient on line $7$.

\begin{center}
\begin{minipage}{0.7 \textwidth}
\IncMargin{0.04in}
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{HyperParameters}{Hyper-parameters}

    \small
    \Input{\quad $\textrm{current weights}\ x$, $\textrm{Langevin iterations}\ L$}
    \HyperParameters{\quad $\textrm{scope}\ \g$, $\textrm{learning rate}\ \eta$, $\textrm{SGLD step size}\ \eta'$}
    \vspace{0.1in}
    \nonl \textrm{// SGLD iterations}\;
    \vspace{0.025in}
    $x', \mu \la x$\;
    \For{$\ell \leq L$}
    {
        $\minibatch{\ell} \la \textrm{sample mini-batch}$\;
        \vspace{0.03in}
        $dx' \la \f{1}{m}\ \sum_{i=1}^m\ \nabla_{x'} f\rbrac{x';\ \xi_{\ell_i}} - \g\ \rbrac{x - x'}$\;
        \vspace{0.03in}
        $x' \la x' - \eta'\ dx' + \sqrt{\eta'}\ \e\ \trm{N}(0, \trm{I})$\;
        \vspace{0.03in}
        $\mu \la (1-\a) \mu + \a\ x'$\;
    }
    \vspace{0.1in}
    \nonl \textrm{// Update weights}\;
    \vspace{0.03in}
    $x \la x - \eta\ \g\ (x-\mu)$
    \caption{$\entropysgd$ algorithm}
    \label{alg:entropysgd}
\end{algorithm}
\DecMargin{0.04in}
\end{minipage}
\end{center}

Let us note that although we have written Alg.~\ref{alg:entropysgd} in the classical SGD setup, we can easily modify it to include techniques such as momentum and gradient pre-conditioning~\citep{duchi2011adaptive} by changing lines $5$ and $9$. In our experiments, we use Nesterov's momentum~\citep{sutskever2013importance} and Adam for outer and inner loops with similar qualitative results.

\todo{
We set the number of SGLD iterations $L$ to $5$-$20$ depending upon the complexity of the dataset, e.g., for experiments on convolutional networks on MNIST and CIFAR-10, we use $L = 20$ while for experiments on a fully-connected network on MNIST $L=5$ works well (cf. Sec.~\ref{s:expt}). We perform exponential averaging of the Langevin iterates $x'$ to estimate $\mu$ with a parameter $\a = 0.9$ so as to put more weight on later samples. In order to quickly transition to the Langevin dynamics phase of SGLD and ensure the we are sampling from the posterior even for small values of $L$, we set the learning rate $\eta'$ on line $5$ to be small factor of the outer SGD's learning rate: $\eta' = 0.1\ \eta$~\citep{welling2011bayesian}.}


\subsubsection{Scoping of $\gamma$}
\label{ss:scoping}

Our scoping mechanism introduced in Sec.~\ref{ss:scoping} increases the value of $\g$ as training progresses in order to focus on more local neighborhoods. 

The ``reverse-annealing'' of the scope $\g$ typically interferes with the learning rate annealing that is popular in deep learning, this is a direct consequence of the update step on Line 7 of Alg.~\ref{alg:entropysgd}. In practice, we therefore scale the local entropy gradient by $\g$ before the update and modify the line to read
$$
    x \leftarrow x - \eta (x - \mu).
$$
We found this to be very effective in simplifying hyper-parameter tuning.


\subsection{Theoretical Properties}
\label{ss:theoretical_properties}

\pc{We can show that $\entropysgd$ results in a smoother loss function and obtains better generalization error than the original objective~\eqref{eq:dnn_objective} if trained for the same number of iterations. Proofs are deferred to the Appendix (see Sec.~\ref{s:app:proofs}).}{clarify that the analysis here is only a sketch, it is not rigorous}

With some overload of notation, we assume that the original loss $f(x)$ is $\b$-smooth, i.e., for all $x,y \in \reals^n$, we have $\norm{\nabla f(x) - \nabla f(y)} \leq \b\ \norm{x-y}$. We additionally assume for the purpose of analysis that no eigenvalue of the Hessian $\nabla^2 f(x)$ lies in the set $[-2 \g-c, c]$ for some small $c > 0$.
%
\begin{lemma}
\label{lem:smoothness_reduction}
The objective $F(x, \g;\ \Xi)$ in~\eqref{eq:entropysgd_objective} is $\f{\a}{1 + \g^{-1}\ c}$-Lipschitz and $\f{\b}{1 + \g^{-1}\ c}$-smooth.
\end{lemma}
The local entropy objective is thus smoother than the original objective while the modified objective in~\eqref{eq:entropysgd_objective} results in a factor of $\r + \f{1}{1 + \g^{-1}\ c}$ in the above lemma.

Let us now obtain a bound on the improvement in generalization error. We denote an optimization algorithm, viz., SGD or $\entropysgd$ by $A(\Xi)$, it is a function of the dataset $\Xi$ and outputs the parameters $x^*$ upon termination. Stability of the algorithm~\citep{bousquet2002stability} is then a notion of how much its output differs in loss upon being presented with two datasets $\Xi$ and $\Xi'$ that differ in at most one sample,
$$
    \sup_{\xi\ \in\ \Xi\ \cup\ \Xi'}\ \sqbrac{ f \rbrac{A(\Xi), \xi} - f \rbrac{A(\Xi'), \xi}} \leq \e.
$$
\citet{hardt2015train} connect uniform stability to generalization error and show that an $\e$-stable algorithm $A(\Xi)$ has generalization error bounded by $\e$, i.e., if $A(\Xi)$ terminates with parameters $x^*$,
$$
    \abs{ \E_{\Xi} \rbrac{R_\Xi(x^*) - R(x^*)} } \leq \e;
$$
where the left hand side is the generalization error: it is the difference between the empirical loss $R_\Xi(x) := \f{1}{N}\ \sum_{k=1}^N\ f(x, \xi_k)$ and the population loss $R(x) := \E_{\xi}\ f(x, \xi)$.

We now employ the following theorem that bounds the stability of an optimization algorithm through the smoothness of its loss function and the number of iterations on the training set.
\begin{theorem}[\citet{hardt2015train}]
\label{thm:generalization_bound_nonconvex}
For an $\a$-Lipschitz and $\b$-smooth loss function, if SGD converges in $T$ iterations on $N$ samples with decreasing learning rate $\eta_t \leq 1/t$ the stability is bounded by
\begin{align*}
    \e
    %&\leq \f{1}{N-1}\ \rbrac{1 + \f{1}{\b c}}\ \rbrac{2 c \a^2}^{\f{1}{1 + \b c}}\ T^{\f{\b c}{1 + \b c}}\\
    &\lessapprox \f{1}{N}\ \a^{1/(1 + \b)}\ T^{1 - 1/(1 + \b)}.
\end{align*}
\end{theorem}
%
Using Lem.~\ref{lem:smoothness_reduction} and Thm.~\ref{thm:generalization_bound_nonconvex} we have
\begin{equation}
    \e_{\ \entropysgd} \lessapprox \rbrac{\a\ T^{-1}}^{\rbrac{1 - \f{1}{1 + \g^{-1}c}}\ \b}\ \e_{\ \textrm{SGD}},
    \label{eq:stability_bound}
\end{equation}
which shows that $\entropysgd$ generalizes better than SGD for all $T > \a$.

As an aside, it is easy to see from the proof of Lem.~\ref{lem:smoothness_reduction} that for a convex loss function $f(x)$, adding the local entropy term does not change the minimizer of the original problem.


\section{Connection to variational inference}
\label{s:app:connection_variational}
The local entropy based objective in $\entropysgd$ has similar motivations as variational inference.

\begin{remark}
discuss the eigenvalue assumption here
\end{remark}

\section{Experiments}
\label{s:expt}

In Sec.~\ref{ss:expt:universality}, we discuss experiments that suggest that the characteristics of the energy landscape around local minimal accessible by SGD are universal to deep architectures.  This observation motivates the $\entropysgd$ algorithm considered in this paper and in Sec.~\ref{ss:expt:mnist} and~\ref{ss:expt:cifar}, we present experimental results on two standard image classification datasets, viz., MNIST and CIFAR-10.

\subsection{Universality of the Hessian at local minima}
\label{ss:expt:universality}

We use automatic differentiation\footnote{\href{https://github.com/HIPS/autograd}{https://github.com/HIPS/autograd}} to compute the Hessian at a local minimum obtained at the end of training for the following networks:

\begin{enumerate}[(i)]
\item \textbf{small-LeNet on MNIST}: This network has $47,658$ parameters and is similar to $\lenet$ but with $10$ and $20$ channels respectively in the first two convolutional layers and $128$ hidden units in the fully-connected layer. We train this with Adam to obtain a test error of $2.4\%$.
%
\item \textbf{small-$\mnistfc$ on MNIST}: A fully-connected network ($50,890$ parameters) with one layer of $32$ hidden units, ReLU non-linearities and cross-entropy loss; it converges to a test error of $2.5\%$ with momentum-based SGD.
%
\item \textbf{char-lstm for text generation}: This is a recurrent network with $48$ hidden units and Long Short-Term Memory (LSTM) architecture~\citep{hochreiter1997long}. It has $32,640$ parameters and we train it with Adam to re-generate a small piece of text consisting of $256$ lines of length $32$ each and $96$-bit one-hot encoded characters.
%
\item \textbf{$\allcnn$ on CIFAR-10}: This is similar to the All-CNN-C network~\citep{springenberg2014striving} with $\approx 1.6$ million weights (cf. Sec.~\ref{ss:expt:cifar}) which we train using Adam to obtain an error of $11.2\%$. Exact Hessian computation is in this case expensive and thus we instead compute the diagonal of the Fisher information matrix~\citep{wasserman2013all} using the element-wise first and second moments of the gradients that Adam maintains, i.e., $\trm{diag} (I) = \E(g^2) - (\E\ g)^2$ where $g$ is the back-propagated gradient. Fisher information measures the sensitivity of the log-likelihood of data given parameters in a neighborhood of a local minimum and thus is exactly equal to the Hessian of the negative log-likelihood. We will consider the diagonal of the empirical Fisher information matrix as a proxy for the eigenvalues of the Hessian, as is common in the literature.
\end{enumerate}

We choose to compute the exact Hessian and to keep the computational and memory requirements manageable, the first three networks considered above are smaller than standard deep networks used in practice. For the last network, we sacrifice the exact computation and instead approximate the Hessian of a large deep network. We note that recovering an approximate Hessian from Hessian-vector products~\citep{pearlmutter1994fast} could be a viable strategy for medium-scale networks.

Fig.~\ref{fig:lenet_hessian} in the introductory Sec.~\ref{s:intro} shows the eigenspectrum of the Hessian for small-LeNet while Fig.~\ref{fig:universality} shows the eigenspectra for the other three networks.
%
A large proportion of eigenvalues of the Hessian are very close to zero or positive with a very small (relative) magnitude. This is suggests that the local geometry of the energy landscape is almost flat at local minima discovered by gradient descent. This agrees with theoretical results such as~\citet{baldassi2016local} where the authors predict that flat regions of the landscape generalize better. Standard regularizers in deep learning such as convolutions, max-pooling and dropout seem to bias SGD towards flatter regions in the energy landscape.
%
The right tails of the eigenspectra are much longer than the left tails. Indeed, as discussed in numerous places in literature~\citep{Bray2007,dauphin2014identifying,spinglass2015}, SGD finds low-index critical points, i.e., optimizers with few negative eigenvalues of the Hessian. What is interesting and novel is that the directions of descent that SGD misses do not have a large curvature.

\begin{figure}[!tbh]
\centering
    \begin{subfigure}[t]{0.44 \textwidth}
        \includegraphics[width=1.08\textwidth]{mnistfc_hessian.pdf}
        \caption{\small $\smallmnistfc$ ($2$ runs): Peak (clipped here) at zero ($\abs{\lambda} \leq 10^{-2}$) accounts for $90\%$ of the entries.\vspace{0.15in}}
        \label{fig:mnistfc_hessian}
    \end{subfigure}
    \hspace{0.2in}
    \begin{subfigure}[t]{0.44 \textwidth}
        \includegraphics[width=\textwidth]{charlstm_hessian.pdf}
        \caption{\small $\charlstm$ ($5$ runs): Almost $95\%$ eigenvalues have absolute value below $10^{-5}$.}
        \label{fig:charlstm_hessian}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \begin{subfigure}[b]{0.44 \textwidth}
        \includegraphics[width=\textwidth]{allcnn_dm.pdf}
        \end{subfigure}
        \hspace{0.2in}
        \begin{subfigure}[b]{0.44 \textwidth}
        \includegraphics[width=\textwidth]{allcnn_dp.pdf}
        \end{subfigure}
    \caption{\small Negative and positive eigenvalues of the Fisher information matrix of $\allcnn$ at a local minimum ($4$ independent runs). The origin has a large peak with $\approx 95\%$ near-zero ($\abs{\lambda} \leq 10^{-5}$) eigenvalues (clipped here).}
    \label{fig:allcnn_hessian}
    \end{subfigure}
\caption{\small Universality of the Hessian: for a wide variety of network architectures, sizes and datasets, optima obtained by SGD are mostly flat (large peak near zero), they always have a few directions with large positive curvature (long positive tails). A very small fraction of directions have negative curvature, and the magnitude of this curvature is extremely small (short negative tails).}
\label{fig:universality}
\end{figure}

\subsection{MNIST}
\label{ss:expt:mnist}

We consider two prototypical networks: the first is a fully-connected network with two hidden layers of $1024$ units each which we denote as ``$\mnistfc$'' and the second is a convolutional neural network with the same size as $\lenet$ but with batch-normalization~\citep{ioffe2015batch}; both with a dropout of probability $0.5$ after each layer. As a baseline, we train for $100$ epochs with Adam and a learning rate of $10^{-3}$ that drops by a factor of $5$ after every $30$ epochs to obtain an average error of $1.39 \pm 0.03\%$ and $0.51 \pm 0.01 \%$ for $\mnistfc$ and $\lenet$ respectively over $5$ independent runs.

For both these networks, we train $\entropysgd$ for $5$ epochs with $L = 20$ and reduce the dropout probability ($0.15$ for $\mnistfc$ and $0.25$ for $\lenet$). The learning rate of the SGLD updates is fixed to $\eta' = 0.1$ while the outer loop's learning rate is set to $\eta=1$ and drops by a factor of $10$ after the second epoch; we use Nesterov's momentum for both the loops. The thermal noise in SGLD updates (Line 5 of Alg.~\ref{alg:entropysgd}) is set to $10^{-3}$. We use an exponentially increasing value of $\g$ for scoping, the initial value of the scope is set to $\g = 10^{-4}$ and this increases by a factor of $1.001$ after each parameter update. The results in Fig.~\ref{fig:mnistfc_test} and Fig.~\ref{fig:lenet_test} show that $\entropysgd$ obtains a comparable generalization error: $1.37 \pm 0.03 \%$ and $0.50 \pm 0.01 \%$, for $\mnistfc$ and $\lenet$ respectively.

\textbf{Computational complexity:} \pc{Since $\entropysgd$ runs $L$ steps of SGLD before each parameter update, the effective number of iterations over the dataset is $L$ times that of SGD or Adam for the same number of parameter updates. We therefore plot the error curves of $\entropysgd$ in Fig.~\ref{fig:mnist_test} and~\ref{fig:allcnn} against the ``effective number of epochs'', i.e. by multiplying the abscissae by a factor of $L$. Note that $L = 1$ for SGD or Adam. The number of effective epochs 

}{add wall-clock times}

\begin{figure}[htp!]
\centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=1.02\textwidth]{mnistfc_valid.pdf}
        \caption{\small $\mnistfc$: Validation error}
        \label{fig:mnistfc_test}
    \end{subfigure}
    \hspace{0.2in}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{lenet_valid.pdf}
        \caption{\small $\lenet$: Validation error}
        \label{fig:lenet_test}
    \end{subfigure}
\caption{\small Comparison of $\entropysgd$ vs. Adam on MNIST\vspace*{0.15in}}
\label{fig:mnist_test}
\end{figure}

\subsection{CIFAR-10}
\label{ss:expt:cifar}

\begin{figure}[htp!]
\centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=1.02\textwidth]{allcnn_loss.pdf}
        \caption{\small $\allcnn$: Training loss}
        \label{fig:allcnn_loss}
    \end{subfigure}
    \hspace{0.2in}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{allcnn_valid.pdf}
        \caption{\small $\allcnn$: Validation error}
        \label{fig:allcnn_valid}
    \end{subfigure}
\caption{\small Comparison of $\entropysgd$ vs. SGD on CIFAR-10}
\label{fig:allcnn}
\end{figure}

We train on CIFAR-10 without data augmentation after performing global contrast normalization and ZCA whitening~\citep{goodfellow2013maxout}. We consider the All-CNN-C network of~\citet{springenberg2014striving} with a batch normalization layer after each convolutional layer and all other architectural parameters such as filter-sizes and weight decay are kept the same. We train for $200$ epochs with SGD and Nesterov's momentum during which the initial learning rate of $0.1$ decreases by a factor of $5$ after every $60$ epochs. We obtain an average error of $8.30 \pm 0.1\%$ over $5$ runs of $200$ epochs vs.\ $9.08\%$ error in $350$ epochs as the authors in~\citet{springenberg2014striving} report and this is thus a very competitive baseline for this network. Let us note that the best result in the literature on non-augmented CIFAR-10 is the ELU-network by~\citet{clevert2015fast} with $6.55\%$ test error.

We train $\entropysgd$ with $L = 20$ for $10$ epochs with the original dropout of $0.5$. The initial learning rate of the outer loop is set to $\eta = 1$ and drops by a factor of $5$ every $4$ epochs, while the learning rate of the SGLD updates is fixed to $\eta' = 0.1$ with thermal noise $\e = 10^{-4}$. As the scoping scheme, we set the initial value of the scope to $\g = 0.03$ which increases by a factor of $1.001$ after each parameter update.
%
Fig.~\ref{fig:allcnn} shows the training and validation error curves for $\entropysgd$ compared with SGD. It shows that local entropy performs as well as SGD on a large CNN; we obtain a validation error of $8.28 \pm 0.11 \%$ in the same number of effective epochs.

We see almost no plateauing of training loss or validation error for $\entropysgd$ in Fig.~\ref{fig:allcnn_loss}; this trait is shared across different networks and datasets in our experiments. Moreover, while the magnitude of the gradients is similar for both SGD and $\entropysgd$ --- in fact, we use this as a way to tune SGLD's hyper-parameters $\eta'$ and $\g$ --- we can use significantly larger learning rates for $\entropysgd$. SGD with large learning rates, say $\eta = 1$ for the above network, would not train at all. These two observations are a direct consequence of the additional smoothness of the local entropy landscape.


\paragraph{Comparison with SGLD:}
\todo{A common way to train Bayesian neural networks is through MCMC algorithms to approximate the likelihood of data marginalized over the weights. Indeed, SGLD which forms the inner loop of $\entropysgd$ is a popular algorithm for variational inference~\citep{balan2015bayesian}. We run vanilla SGLD for two prototypical networks in Sec.~\ref{s:expt}, $\lenet$ on MNIST and $\allcnn$ on CIFAR-10.}

\subsection{Recurrent neural networks}
\label{ss:expt:rnn}

In order to test the efficacy of $\entropysgd$ on recurrent architectures, we train an LSTM network on the Penn Tree Bank (PTB) dataset for word-level text prediction. This dataset contains about one million words divided into a training set of about $930,000$ words, a validation set of $74,000$ words and $82,000$ words with a vocabulary of size $10,000$. As a baseline, we follow the experiments in~\citet{zaremba2014recurrent}. Our network called $\ptblstm$ consists of two layers with $1500$ hidden units, each unrolled for $35$ time steps and a dropout of probability $0.65$; note that this is a large network with about $66$ million weights. It is trained with SGD (without momentum, like the original authors) for $55$ epochs with a learning rate $\eta = 1$ for the first $14$ epochs that is annealed by a factor of $1.15$ after every epoch. \pc{We obtained a word perplexity of $81.125$ on the validation set and $78.232$}{std-dev} on the test set with this setup, which closely matches the results of the original authors.

We run $\entropysgd$ on $\ptblstm$ for $5$ epochs with $L = 5$; note that this results in only $25$ effective epochs. We do not use scoping for this network and instead fix $\g = 10^{-3}$. The initial learning rate of the outer loop is $\eta = 1$ which reduces by a factor of $10$ at each epoch after the third epoch. The SGLD learning rate is fixed to $\eta' = 1$ with $\e = 10^{-4}$. We obtain a word perplexity of $80.116 \pm 0.069$ on the validation set and $77.656 \pm 0.171$ on the test set. As Fig.~\ref{fig:ptb_valid} shows, $\entropysgd$ trains significantly faster than SGD and also achieves a slightly better generalization perplexity.

Next, we train an LSTM to perform character-level text-prediction. As a dataset, following the experiments of~\citet{karpathy2015visualizing}, we use the text of War and Peace by Leo Tolstoy which contains about $3$ million characters divided into training ($80\%$), validation ($10\%$) and test ($10\%$) sets. We use an LSTM consisting of two layers of $128$ hidden units unrolled for $50$ time steps and a vocabulary of size $87$. We train the baseline with Adam for $50$ epochs with an initial learning rate of $0.002$ that decays by a factor of $2$ after every $5$ epochs to obtain a validation perplexity of $1.224 \pm 0.008$ and a test perplexity of \pc{$1.227 \pm 0.006$}{this is a place-holder}.

As noted in Sec.~\ref{ss:alg}, we can easily wrap Alg.~\ref{alg:entropysgd} inside other variants of SGD such as Adam; indeed this involves simply substituting the local entropy gradient in place of the usual back-propagated gradient. For this experiment, we construct $\entropyadam$ which is equivalent to Adam that uses the local entropy gradient (computed via SGLD). We run $\entropyadam$ for $5$ epochs with $L = 5$ and a fixed $\g = 0.01$ with an initial learning rate of $0.01$ that decreases by a factor of $2$ at each epoch. Note that this again results in only $25$ effective epochs. We obtain a validation perplexity of $1.213 \pm 0.007$ and a test perplexity of \pc{$1.20 \pm 00$}{this is a place-holder} over $4$ independent runs which is slightly better than the baseline, in half as much wall-clock time. We plot the error curves in Fig.~\ref{fig:char_valid} for this experiment.

Tuning the momentum in $\entropysgd$ was crucial to getting good results on RNNs. While the SGD baseline on $\ptblstm$ does not use momentum (and in fact, does not train well with momentum) we used a momentum of $0.5$ for $\entropysgd$. On the other hand, the baseline for $\charlstm$ was trained with Adam with $\b_1 = 0.9$ ($\b_1$ in Adam controls the moving average of the gradient) while we set $\b_1 = 0.5$ for $\entropyadam$. In contrast to this observation about RNNs, all our experiments on CNNs used a momentum of $0.9$. \pc{This is also corroborated by monitoring the angle between the local entropy gradient and the vanilla SGD gradient during training. This angle changes much more rapidly for RNNs than for CNNs which suggests a more rugged energy landscape for the former.}{}

\todo{the two plots below are place holders}
\begin{figure}[htp!]
\centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{allcnn_valid.pdf}
        \caption{\small $\ptblstm$: Validation, test perplexity}
        \label{fig:ptb_valid}
    \end{subfigure}
    \hspace{0.2in}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=1.02\textwidth]{allcnn_loss.pdf}
        \caption{\small $\charlstm$: Validation perplexity}
        \label{fig:char_valid}
    \end{subfigure}
\caption{\small Comparison of $\entropysgd$ vs. SGD / Adam on RNNs}
\label{fig:rnn}
\end{figure}

\section{Discussion}
\label{s:discussion}

In our experiments, $\entropysgd$ results in a comparable generalization error as SGD, but always has a lower cross-entropy loss on the training set. This suggests the following in the context of energy landscapes of deep networks. Roughly, wide valleys favored by $\entropysgd$ are located deeper in the landscape with a lower empirical loss than local minima discovered by SGD where it presumably gets stuck. Such an interpretation is contrast to theoretical models of deep networks (cf.\@ Sec.~\ref{s:prior_work}) which predicts multiple equivalent local minima with the same loss. Our work suggests that geometric properties of the energy landscape are crucial to generalize well and provides algorithmic approaches to exploit them. However, the literature lacks general results about the geometry of the loss functions of deep networks --- convolutional neural networks in particular --- and this is a promising direction for future work.

\pc{If we focus on the inner loop of the algorithm, SGLD updates compute the average gradient (with Langevin noise) in a neighborhood of the parameters while maintaining the Polyak average of the new parameters. Such an interpretation is very close to averaged SGD of~\citet{polyak1992acceleration} and~\citet{bottou2012stochastic} and worth further study. Moreover, our experiments show that $\entropysgd$ trains significantly faster than SGD for recurrent networks with relatively minor gains in \fix{wall-clock time for CNNs}{}. Estimating the gradient of local entropy cheaply with few SGLD iterations, or more interestingly, by using a smaller network in the teacher-student fashion~\citep{balan2015bayesian} is another avenue for extensions to this work.}{don't want to give away the farm}

\section{Conclusions}
\label{s:conclusions}

We introduced an algorithm named $\entropysgd$ for optimization of deep networks. This was motivated from the observation that the energy landscape near a local minimum discovered by SGD is almost flat for a wide variety of deep networks irrespective of their architecture, input data or training methods. We connected this observation to the concept of local entropy which we used to bias the optimization towards flat regions that have low generalization error. Our experiments showed that this algorithm is applicable to large convolutional and recurrent deep networks used in practice.

\section{Acknowledgments}
\label{s:acknowledgements}

This work was supported by the Office of Naval Research (ONR), Air Force Office of Scientific Research (AFOSR) and Army Research Office (ARO).

{
\footnotesize
\linespread{0.8}
\bibliographystyle{iclr2017_conference}
\bibliography{chaudhari.choromanska.ea.iclr17}
}

\begin{appendices}

\renewcommand\thetable{\thesection\arabic{table}}
\renewcommand\thefigure{\thesection\arabic{figure}}

\section{Stochastic gradient Langevin dynamics (SGLD)}
\label{s:app:langevin}

Local entropy in Def.~\eqref{def:local_entropy} is an expectation over the entire configuration space $x \in \reals^n$ and is hard to compute, we can however approximate its gradient using Markov chain Monte-Carlo (MCMC) techniques. In this section, we briefly review stochastic gradient Langevin dynamics~\citep{welling2011bayesian} that is an MCMC algorithm designed to draw samples from a Bayesian posterior and scales to large datasets using mini-batch updates.

For a parameter vector $x \in \reals^n$ with a prior distribution $p(x)$ and if the probability of generating a data item $\xi_k$ given a model parameterized by $x$ is $p(\xi_k \given x)$, the posterior distribution of the parameters based on $N$ data items can be written as
\begin{equation}
    p\rbrac{x \given \xi_{\ k \leq N}}\ \propto\ p(x)\ \prod_{k=1}^N\ p\rbrac{\xi_k \given x}.
    \label{eq:bayesian_posterior}
\end{equation}
Langevin dynamics~\citep{neal2011mcmc} injects Gaussian noise into maximum-a-posteriori (MAP) updates to prevent over-fitting the solution $x^*$ of the above equation. The updates can be written as
\begin{equation}
    \Delta x_t = \f{\eta}{2}\ \rbrac{ \nabla \log p(x_t) + \sum_{k=1}^N\ \nabla p(\xi_k \given x_t)} + \sqrt{\eta}\ \e_t;
    \label{eq:langevin_bayesian}
\end{equation}
where $\e_t \sim \trm{N}(0, 1)$ is Gaussian noise and $\eta$ is the learning rate. In this form, Langevin dynamics faces two major hurdles for applications to large datasets. First, computing the gradient $\sum_{k=1}^N\ \nabla p(\xi_k \given x_t)$ over all samples for each update $\Delta x_t$ becomes prohibitive. However, as~\citet{welling2011bayesian} show, one can instead simply use the average gradient over $m$ data samples (mini-batch) as follows:
\begin{equation}
    \Delta x_t = \f{\eta_t}{2}\ \rbrac{ \nabla \log p(x_t) + \f{N}{m}\ \sum_{k=1}^m\ \nabla p(\xi_k \given x_t)} + \sqrt{\eta_t}\ \e_t.
    \label{eq:sgld_bayesian}
\end{equation}
Secondly, Langevin dynamics in~\eqref{eq:langevin_bayesian} is the discrete-time approximation of a continuous-time stochastic differential equation~\citep{mandt2016variational} thereby necessitating a Metropolis-Hastings (MH) rejection step~\citep{roberts2002langevin} which again requires computing $p(\xi_k \given x)$ over the entire dataset. However, if the learning rate $\eta_t \to 0$, we can also forgo the MH step~\citep{chen2014stochastic}. \citet{welling2011bayesian} also argue that the sequence of samples $x_t$ generated by updating~\eqref{eq:sgld_bayesian} converges to the correct posterior~\eqref{eq:bayesian_posterior} and one can hence compute the statistics of any function $g(x)$ of the parameters using these samples. Concretely, the posterior expectation $\E\sqbrac{g(x)}$ is given by
$
    \E\sqbrac{g(x)} \approx \f{\sum_{s=1}^t \eta_t\ g(x_t)}{\sum_{s=1}^t \eta_t};
$
which is the average computed by weighing each sample by the corresponding learning rate in~\eqref{eq:sgld_bayesian}. In this paper, we will consider a uniform prior on the parameters $x$ and hence the first term in~\eqref{eq:sgld_bayesian}, viz., $\nabla \log p(x_t)$ vanishes.

Let us note that there is a variety of increasingly sophisticated MCMC algorithms applicable to our problem, e.g., Hamiltonian Monte Carlo (SGHMC) by~\citet{chen2014stochastic} based on volume preserving flows in the ``parameter-momentum'' space, stochastic annealing thermostats (Santa) by~\citet{chen2015bridging} etc. We can also employ these techniques, although we use SGLD for ease of implementation. The authors in~\citet{ma2015complete} provide an elaborate overview of these methods.

\section{Proofs}
\label{s:app:proofs}

\begin{proof}[Proof sketch of Lemma~\ref{lem:smoothness_reduction}]
The gradient $-\nabla F(x)$ is computed in Sec.~\ref{ss:grad_local_entropy} to be $\g\ \rbrac{x - \ag{x';\ \minibatch{\ell}}}$. Consider the term
\begin{align*}
    x - \ag{x';\ x}
    &= x - Z_{x,\g}^{-1}\ \int_{x'}\ x'\ e^{-f(x') - \f{\g}{2}\ \norm{x-x'}^2}\ dx'\\
    &\approx x - Z_{x,\g}^{-1}\ \int_{s}\ (x + s)\ e^{-f(x) - \nabla f(x)^\top s - \f{1}{2}\ s^\top \rbrac{\g + \nabla^2 f(x)} s}\ ds\\
    &= x\rbrac{1 -  Z_{x,\g}^{-1}\ \int_{s}\ e^{-f(x) - \nabla f(x)^\top s - \f{1}{2}\ s^\top \rbrac{\g + \nabla^2 f(x)} s}\ ds} - Z_{x,\g}^{-1}\ \int_{s}\ s\ e^{-f(x) - \nabla f(x)^\top s - \f{1}{2}\ s^\top \rbrac{\g + \nabla^2 f(x)} s}\ ds\\
    &= -Z_{x,\g}^{-1}\ e^{-f(x)}\ \int_{s}\ s\ e^{-\nabla f(x)^\top s - \f{1}{2}\ s^\top \rbrac{\g + \nabla^2 f(x)} s}\ ds.
\end{align*}
The above expression is the mean of a distribution $\propto e^{-\nabla f(x)^\top s - \f{1}{2}\ s^\top \rbrac{\g + \nabla^2 f(x)} s}$. We can approximate it using the saddle point method as the value of $s$ that minimizes the exponent to get
$$
    x - \ag{x';\ x} \approx \rbrac{\nabla^2 f(x) + \g\ I}^{-1}\ \nabla f(x).
$$
Let us denote $A(x) := \rbrac{I + \g^{-1}\ \nabla^2 f(x)}^{-1}$. Plugging this into the condition for smoothness, we have
\begin{align*}
    \norm{\nabla F(x, \g) - \nabla F(y, \g)}
    &= \norm{A(x)\ \nabla f(x) - A(y)\ \nabla f(y)}\\
    &\leq \rbrac{\sup_x\ \norm{A(x)}}\ \b\ \norm{x-y}.
\end{align*}
Unfortunately, we can only get a uniform bound if we assume that for a small constant $c > 0$, no eigenvalue of $\nabla^2 f(x)$ lies in the set $[-2 \g -c, c]$. This gives
$$
    \rbrac{\sup_x\ \norm{A(x)}} \leq \f{1}{1+\g^{-1}\ c}.
$$
This shows that a smaller value of $\g$ results in a smoother energy landscape, except at places with very flat directions. The Lipschitz constant also decreases by the same factor.
\end{proof}

\ignore{
\section{Sub-dominant clusters in a spin glass}
\label{s:app:spin_glass}

As an pedagogical example of a local entropy based loss function, we first consider the optimization of a well-studied model in statistical physics known as a mixed spherical spin glass~\citep{talagrand2003spin}. For $J^2_{ijk},\ J^3_{ijk} \sim \trm{N}(0,1)$, the energy of a spin glass is given by
\begin{equation}
    H(x) = \f{1}{10\ \sqrt{n}}\ \sum_{i,j=1}^n\ J^2_{ij}\ x_i\ x_j + \f{1}{n} \sum_{i,j,k=1}^n\ J^3_{ijk}\ x_i\ x_j\ x_k
    \label{eq:sg_hamiltonian}
\end{equation}
where the parameter vector $x \in \reals^n$ is called the ``spin''. A spherical spin glass is a Gaussian field on the sphere, i.e., we have an additional constraint $\norm{x}^2_2 = n$ or $x \in S^{n-1}(\sqrt{n})$. We are interested in computing
\begin{equation}
    x^*_H = \argmin_{x \in S^{n-1}(\sqrt{n})}\ H(x).
    \label{eq:3sg}
\end{equation}
This is a non-convex problem and in general, hard; the number of local minima and saddle points of $H(x)$ scale exponentially in $n$~\citep{auffinger2013random}. However, using results from statistical physics, one can show that all local minima concentrate around the global minimum~\citep{panchenko2013sherrington}:
$$
    \lim_{n \to \infty}\ \inf_{x \in S^{n-1}(\sqrt{n})}\ \f{H(x)}{n} \approx -1.8227.
$$
We have chosen a mixed spin glass in~\eqref{eq:sg_hamiltonian} to ensure that the Hamiltonian has both deep, narrow local minima belonging to the $3$-spin term $x_i x_j x_k$ and wide, shallower valleys resulting from the $2$-spin term $x_i x_j$ with a small curvature.
%Moreover, since the Hamiltonian is isotropic, all local minima have an almost equivalent local geometry of the energy landscape, in particular, the Euclidean Hessian $\nabla^2 H(x) \in \reals^{(n-1) \times (n-1)}$ belongs to the Gaussian Orthogonal Ensemble (modulo a constant).
Our modified optimization problem with a negative local entropy term is given by
\begin{equation}
    x^*_f = \argmin_{x \in S^{n-1}(\sqrt{n})}\ \rho H(x) - F(x, \g);
    \label{eq:3sg_modified}
\end{equation}
with $\rho = 0.01$ and $\g = 10^{-4}$ and the local entropy $F(x, \g)$ is the log-partition function of
$$
    \P(x';\ x, \g) \propto \exp \rbrac{-H(x') - \f{\g}{2}\ x^\top\ x'}
$$
is estimated using Langevin dynamics with $100$ iterations in Alg.~\ref{alg:entropysgd}. In a minor deviation from the development in Sec.~\ref{s:entropysgd}, we use the dot product $x^\top x'$ as the biasing term. With the spherical constraint $\norm{x}^2_2 = n$ in play, these two methods are equivalent.

For both~\eqref{eq:3sg} and~\eqref{eq:3sg_modified} we set $n = 100$ and perform $10^3$ Monte-Carlo runs from uniformly randomly sampled initial conditions on the hyper-sphere with a learning rate $\eta = 0.01$ until the norm of the gradient is smaller than $10^{-4}$. We then contrast the eigenspectrum of the original Hessian, viz., $\nabla^2 H(x)$ (projected onto the tangent space of the sphere) for both these problems.

Fig.~\ref{fig:expt:sg} shows the results of this simulation. As expected, the solutions of~\eqref{eq:3sg} have energies close to the asymptotic value $-1.657$. The energies of the optimizers of~\eqref{eq:3sgentropy} are higher, with a mean $-1.45$. On the other hand, the Hessian for~\eqref{eq:3sg} has large positive eigenvalues suggesting deep, sharp local minima while the Hessian for~\eqref{eq:3sgentropy} has much smaller positive eigenvalues, which suggests shallower and wider valleys in the energy landscape. The effect of the local-entropy term is thus to artificially under-optimize the loss function; indeed it avoids deep and sharp valleys in the energy landscape and causes gradient descent to settle in wider --- although shallower --- local minima where nearby spins have similar values of the Hamiltonian $H(x)$. As experiments in Sec.~\ref{ss:expt:mnist} and~\ref{ss:expt:cifar} show, this is crucial to avoid over-fitting on the training set and leads to better generalization.
}

\end{appendices}

\end{document} 